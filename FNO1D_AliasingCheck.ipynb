{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymhhi7mzvd5p",
        "outputId": "33f275db-bdac-47c0-bd16-3bddeb8a65cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AISE'...\n",
            "remote: Enumerating objects: 1265, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 1265 (delta 59), reused 83 (delta 31), pack-reused 1143 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1265/1265), 324.43 MiB | 14.06 MiB/s, done.\n",
            "Resolving deltas: 100% (624/624), done.\n",
            "Filtering content: 100% (10/10), 90.12 MiB | 24.12 MiB/s, done.\n",
            "Encountered 10 file(s) that should have been pointers, but weren't:\n",
            "\tproject_1/checkpoints/all2all/fno_m30_w32_d2_lr0.001_20250102_152429/model.pth\n",
            "\tproject_1/checkpoints/onetoall/fno_m30_w32_d2_lr0.001_20250102_151601/model.pth\n",
            "\tproject_1/checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250102_145624/model.pth\n",
            "\tproject_1/data/test_sol.npy\n",
            "\tproject_1/data/test_sol_OOD.npy\n",
            "\tproject_1/data/test_sol_res_128.npy\n",
            "\tproject_1/data/test_sol_res_32.npy\n",
            "\tproject_1/data/test_sol_res_64.npy\n",
            "\tproject_1/data/test_sol_res_96.npy\n",
            "\tproject_1/data/train_sol.npy\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/youwuyou/AISE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AISE/project_1\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1zfjP8TyPUd",
        "outputId": "85ebfebf-28c2-4d94-caf0-3df7786ae097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AISE/project_1\n",
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  evaluate.py  \u001b[01;34mresults\u001b[0m/                utils.py\n",
            "\u001b[01;34mdata\u001b[0m/         fno.py       train_fno.py            visualization.py\n",
            "dataset.py    README.md    train_fno_with_time.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class OneToOne(Dataset):\n",
        "    def __init__(self,\n",
        "                which,\n",
        "                training_samples=64,\n",
        "                data_path=\"data/train_sol.npy\",\n",
        "                lx=1.0,\n",
        "                dt=0.25,\n",
        "                start_idx=0,\n",
        "                end_idx=4,\n",
        "                device='cuda'):\n",
        "        # dataset = torch.from_numpy(np.load(data_path))\n",
        "        dataset = torch.from_numpy(np.load(data_path)).type(torch.float32)\n",
        "\n",
        "        if device == 'cuda':\n",
        "            dataset = dataset.type(torch.float32).to(device)\n",
        "\n",
        "        if which == \"training\":\n",
        "            self.data = dataset[:training_samples]\n",
        "        elif which == \"validation\":\n",
        "            self.data = dataset[training_samples:]\n",
        "        elif which == \"testing\":\n",
        "            self.data = dataset\n",
        "        else:\n",
        "            raise ValueError(\"Dataset must be 'training', 'validation' or `testing`\")\n",
        "\n",
        "        self.length = len(self.data)\n",
        "        self.dt = dt\n",
        "        self.nt = self.data.shape[1]\n",
        "        num_timesteps = self.nt - 1\n",
        "\n",
        "        # Stack all available timesteps\n",
        "        self.u = torch.stack([self.data[:, i, :] for i in range(self.nt)], dim=1)\n",
        "\n",
        "        # Calculate time derivatives based on available timesteps\n",
        "        # Multiple timesteps - use central differences where possible\n",
        "        self.v = []\n",
        "        for i in range(self.nt - 1):\n",
        "            # print(f\"current i: {i}\")\n",
        "            if i == 0:\n",
        "                # Set initial velocity to zero\n",
        "                deriv = torch.zeros_like(self.u[:, 0])\n",
        "            elif i == self.nt:\n",
        "                # Backward difference for last point\n",
        "                deriv = (self.u[:, -1] - self.u[:, -2]) / self.dt\n",
        "            else:\n",
        "                # Central difference for interior points\n",
        "                deriv = (self.u[:, i+1] - self.u[:, i-1]) / (2 * self.dt)\n",
        "            self.v.append(deriv)\n",
        "        self.v = torch.stack(self.v, dim=1)\n",
        "\n",
        "        # Domain setup\n",
        "        self.lx = lx\n",
        "        self.nx = self.data.shape[-1]\n",
        "        self.x_grid = torch.linspace(0, self.lx, self.nx).to(device)\n",
        "\n",
        "        # Adjust indices if they exceed available timesteps\n",
        "        assert(end_idx >= start_idx)\n",
        "        self.start_idx = start_idx\n",
        "        self.end_idx = end_idx\n",
        "        self.u_start = self.u[:, self.start_idx]\n",
        "        self.v_start = self.v[:, self.start_idx]\n",
        "        self.u_end = self.u[:, self.end_idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        u_start = self.u_start[idx]\n",
        "        v_start = self.v_start[idx]\n",
        "        dt = torch.full_like(u_start, self.dt * (self.end_idx - self.start_idx), device=u_start.device)\n",
        "        x_grid = self.x_grid.to(u_start.device)\n",
        "        input_data = torch.stack((u_start, v_start, x_grid, dt), dim=-1)\n",
        "        output_data = self.u_end[idx].unsqueeze(-1)\n",
        "        return input_data, output_data\n"
      ],
      "metadata": {
        "id": "a3gEOvL7yPRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpectralConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "    The FNO1d uses SpectralConv1d as its crucial part,\n",
        "        - implements the Fourier integral operator in a layer\n",
        "            F‚Åª¬π(R‚ÅΩÀ°‚Åæ‚àòF)(u)\n",
        "        - uses FFT, linear transform, and inverse FFT, applicable to equidistant mesh\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, modes1):\n",
        "        super(SpectralConv1d, self).__init__()\n",
        "        if not isinstance(modes1, int) or modes1 <= 0:\n",
        "            raise ValueError(f\"modes1 must be a positive integer, got {modes1}\")\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes1 = modes1  # Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
        "\n",
        "        self.scale = (1 / (in_channels * out_channels))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
        "\n",
        "    def compl_mul1d(self, input, weights):\n",
        "        \"\"\"\n",
        "        Complex multiplication in 1D\n",
        "        (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
        "        \"\"\"\n",
        "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            1) Compute Fourier coefficients\n",
        "            2) Multiply relevant Fourier modes\n",
        "            3) Transform the data to physical space\n",
        "            HINT: Use torch.fft library torch.fft.rfft\n",
        "        \"\"\"\n",
        "        batchsize = x.shape[0]\n",
        "\n",
        "        # Compute Fourier coefficients up to factor of e^(- something constant)\n",
        "        x_ft = torch.fft.rfft(x)\n",
        "\n",
        "        # Use min to limit modes to what's available\n",
        "        effective_modes = min(self.modes1, x.size(-1) // 2 + 1)\n",
        "\n",
        "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,\n",
        "                        device=x.device, dtype=torch.cfloat)\n",
        "        out_ft[:, :, :effective_modes] = self.compl_mul1d(x_ft[:, :, :effective_modes],\n",
        "                                                    self.weights1[:,:,:effective_modes])\n",
        "\n",
        "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
        "        return x\n",
        "\n",
        "class FNO1d(nn.Module):\n",
        "    def __init__(self, modes, width, depth, device=\"cuda\", nfun=1, padding_frac=1/4, time_dependent=False):\n",
        "        \"\"\"\n",
        "        The overall network ùí¢_Œ∏(u). It contains [depth] layers of the Fourier layers.\n",
        "        Each layer implements:\n",
        "        u_(l+1) = œÉ(K^(l)(u_l) + W^(l)u_l + b^(l))\n",
        "        where:\n",
        "        - K^(l) is the Fourier integral operator F‚Åª¬π(R‚ÅΩÀ°‚Åæ‚àòF)\n",
        "        - W^(l) is the residual connection\n",
        "        - b^(l) is the bias term\n",
        "\n",
        "        Complete architecture:\n",
        "        1. Lift the input to the desired channel dimension by P (self.fc0)\n",
        "        2. Apply [depth] layers of Fourier integral operators with residual connections\n",
        "        3. Project from the channel space to the output space by Q (self.fc1 and self.fc2)\n",
        "\n",
        "        input: the solution of the initial condition and location (a(x), x)\n",
        "        input shape: (batchsize, x=s, c=2)\n",
        "        output: the solution of a later timestep\n",
        "        output shape: (batchsize, x=s, c=1)\n",
        "        \"\"\"\n",
        "        super(FNO1d, self).__init__()\n",
        "\n",
        "        self.modes = modes\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.padding_frac = padding_frac\n",
        "        self.time_dependent = time_dependent\n",
        "\n",
        "        self.fc0 = nn.Linear(nfun + 1, self.width) # +1 for x_grid\n",
        "\n",
        "        # Fourier layers\n",
        "        self.spectral_list = nn.ModuleList([\n",
        "            SpectralConv1d(self.width, self.width, self.modes) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "        # Linear residual connections\n",
        "        self.w_list = nn.ModuleList([\n",
        "            nn.Linear(self.width, self.width, bias=False) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "        # Bias terms\n",
        "        self.b_list = nn.ParameterList([\n",
        "            nn.Parameter(torch.zeros(1, self.width, 1)) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Projection layers\n",
        "        self.fc1 = nn.Linear(self.width, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "        self.to(device)  # Move model to specified device\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, sequence_length, channel]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape [batch_size, sequence_length, 1]\n",
        "        \"\"\"\n",
        "        u_start = x[..., 0].unsqueeze(-1)\n",
        "        v_start = x[..., 1].unsqueeze(-1)\n",
        "        x_grid  = x[..., 2].unsqueeze(-1)\n",
        "\n",
        "        # dt will not be used if time-dependent is not set True\n",
        "        dt = x[..., 3].unsqueeze(-1)  # Keep as [batch]\n",
        "\n",
        "        x = torch.cat((u_start, v_start, x_grid), dim=-1)\n",
        "        x = self.fc0(x)\n",
        "        x = x.permute(0, 2, 1)  # Now shape is [batch, width, sequence_length]\n",
        "\n",
        "        # Apply padding\n",
        "        x_padding = int(round(x.shape[-1] * self.padding_frac))\n",
        "        x = F.pad(x, [0, x_padding])\n",
        "\n",
        "        # Apply Fourier layers with residual connections and bias\n",
        "        for i in range(self.depth):\n",
        "            # Store input for residual connection\n",
        "            x_input = x\n",
        "            # Fourier integral operator K^(l)\n",
        "            x1 = self.spectral_list[i](x)  # Shape: [batch, width, sequence_length]\n",
        "            # Residual connection W^(l)\n",
        "            x2 = self.w_list[i](x_input.transpose(1, 2)).transpose(1, 2)\n",
        "            # Expand b to match the sequence length\n",
        "            b_expanded = self.b_list[i].expand(-1, -1, x1.size(-1))\n",
        "\n",
        "            # Combine with bias: K^(l) + W^(l) + b^(l)\n",
        "            x = x1 + x2 + b_expanded\n",
        "\n",
        "            # Apply time-conditional normalization if time-dependent\n",
        "            if self.time_dependent:\n",
        "                # tensor([[a], [b], [c], [d], [e]])\n",
        "                dt = dt[:, 0].unsqueeze(-1)  # or dt[:, 0].view(-1, 1)\n",
        "                x = self.film_list[i](x, dt)\n",
        "\n",
        "            # Apply activation (except for last layer)\n",
        "            if i != self.depth - 1:\n",
        "                x = self.activation(x)\n",
        "\n",
        "        # Remove padding\n",
        "        x = x[..., :-x_padding]\n",
        "\n",
        "        # Final projection layers Q\n",
        "        x = x.permute(0, 2, 1)  # Back to [batch, sequence_length, width]\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Shape should be [batch_size, sequence_length, 1]\n",
        "\n",
        "    def print_size(self):\n",
        "        nparams = 0\n",
        "        nbytes = 0\n",
        "        for param in self.parameters():\n",
        "            nparams += param.numel()\n",
        "            nbytes += param.data.element_size() * param.numel()\n",
        "        print(f'Total number of model parameters: {nparams}')\n",
        "        return nparams"
      ],
      "metadata": {
        "id": "ZUCFb8HhyPO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedFNO1d(nn.Module):\n",
        "    def __init__(self, modes, width, depth, device=\"cuda\", nfun=1, padding_frac=1/4, time_dependent=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.modes = modes\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.padding_frac = padding_frac\n",
        "        self.time_dependent = time_dependent\n",
        "\n",
        "        self.fc0 = nn.Linear(nfun + 1, self.width) # +1 for x_grid\n",
        "\n",
        "        # Fourier layers\n",
        "        self.value_spectral_list = nn.ModuleList([\n",
        "            SpectralConv1d(self.width, self.width, self.modes) for _ in range(self.depth)\n",
        "        ])\n",
        "        self.gate_spectral_list = nn.ModuleList([\n",
        "            SpectralConv1d(self.width, self.width, self.modes) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "        # Linear residual connections\n",
        "        self.value_w_list = nn.ModuleList([\n",
        "            nn.Linear(self.width, self.width, bias=False) for _ in range(self.depth)\n",
        "        ])\n",
        "        self.gate_w_list = nn.ModuleList([\n",
        "            nn.Linear(self.width, self.width, bias=False) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "        # Bias terms\n",
        "        self.value_b_list = nn.ParameterList([\n",
        "            nn.Parameter(torch.zeros(1, self.width, 1)) for _ in range(self.depth)\n",
        "        ])\n",
        "        self.gate_b_list = nn.ParameterList([\n",
        "            nn.Parameter(torch.zeros(1, self.width, 1)) for _ in range(self.depth)\n",
        "        ])\n",
        "\n",
        "\n",
        "        # Projection layers\n",
        "        self.value_fc1 = nn.Linear(self.width, 128)\n",
        "        self.gate_fc1 = nn.Linear(self.width, 128)\n",
        "        self.value_fc2 = nn.Linear(128, 1)\n",
        "\n",
        "\n",
        "        self.to(device)  # Move model to specified device\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, sequence_length, channel]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape [batch_size, sequence_length, 1]\n",
        "        \"\"\"\n",
        "        u_start = x[..., 0].unsqueeze(-1)\n",
        "        v_start = x[..., 1].unsqueeze(-1)\n",
        "        x_grid  = x[..., 2].unsqueeze(-1)\n",
        "\n",
        "        # dt will not be used if time-dependent is not set True\n",
        "        dt = x[..., 3].unsqueeze(-1)  # Keep as [batch]\n",
        "\n",
        "        x = torch.cat((u_start, v_start, x_grid), dim=-1)\n",
        "        x = self.fc0(x)\n",
        "        x = x.permute(0, 2, 1)  # Now shape is [batch, width, sequence_length]\n",
        "\n",
        "        # Apply padding\n",
        "        x_padding = int(round(x.shape[-1] * self.padding_frac))\n",
        "        gate_x = F.pad(x, [0, x_padding])\n",
        "        value_x = torch.ones_like(gate_x)\n",
        "\n",
        "\n",
        "        # Apply Fourier layers with residual connections and bias\n",
        "        for i in range(self.depth):\n",
        "            # Value Network\n",
        "            value_x1 = self.value_spectral_list[i](value_x)\n",
        "            value_x2 = self.value_w_list[i](value_x.transpose(1, 2)).transpose(1, 2)\n",
        "            value_b_expanded = self.value_b_list[i].expand(-1, -1, value_x1.size(-1))\n",
        "            value_x = value_x1 + value_x2 + value_b_expanded\n",
        "\n",
        "            #Gating Network\n",
        "            gate_x1 = self.gate_spectral_list[i](gate_x)\n",
        "            gate_x2 = self.gate_w_list[i](gate_x.transpose(1, 2)).transpose(1, 2)\n",
        "            gate_b_expanded = self.gate_b_list[i].expand(-1, -1, gate_x1.size(-1))\n",
        "            gate_x = gate_x1 + gate_x2 + gate_b_expanded\n",
        "\n",
        "            if i != self.depth - 1:\n",
        "               value_x = torch.sigmoid(gate_x) * value_x\n",
        "\n",
        "        # Remove padding\n",
        "        value_x = value_x[..., :-x_padding]\n",
        "        gate_x = gate_x[..., :-x_padding]\n",
        "\n",
        "        # Final projection layers Q\n",
        "        value_x = value_x.permute(0, 2, 1)\n",
        "        gate_x = gate_x.permute(0, 2, 1)  # Back to [batch, sequence_length, width]\n",
        "        value_x = self.value_fc1(value_x)\n",
        "        gate_x = self.gate_fc1(gate_x)\n",
        "        x = value_x * torch.sigmoid(gate_x)\n",
        "        x = self.value_fc2(x)\n",
        "        return x  # Shape should be [batch_size, sequence_length, 1]\n",
        "\n",
        "    def print_size(self):\n",
        "        nparams = 0\n",
        "        nbytes = 0\n",
        "        for param in self.parameters():\n",
        "            nparams += param.numel()\n",
        "            nbytes += param.data.element_size() * param.numel()\n",
        "        print(f'Total number of model parameters: {nparams}')\n",
        "        return nparams"
      ],
      "metadata": {
        "id": "5TmcyU2r2HBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from utils import (\n",
        "train_model,\n",
        "get_experiment_name,\n",
        "save_config\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU device:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK2oyt89yPMj",
        "outputId": "f1fa91fc-2fd9-4b77-c264-02360756885b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CUDA available: True\n",
            "GPU device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(0)"
      ],
      "metadata": {
        "id": "c4o968n4yPJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"depth\": 2,\n",
        "    \"modes\": 30,\n",
        "    \"width\": 16,\n",
        "    \"nfun\": 2,\n",
        "    \"time_dependent\": False,\n",
        "    \"device\": device\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "data_mode = \"onetoone\"\n",
        "training_config = {\n",
        "    \"data_mode\": data_mode,\n",
        "    'n_train': 64,\n",
        "    'batch_size': 5,\n",
        "    'learning_rate': 0.0005,\n",
        "    'epochs': 1200,\n",
        "    'step_size': 300,\n",
        "    'gamma': 0.1,\n",
        "    'patience': 80,\n",
        "    'freq_print': 1,\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "# Combine configurations for experiment naming\n",
        "naming_config = {**model_config, 'learning_rate': training_config['learning_rate']}\n",
        "\n",
        "# Create experiment directory\n",
        "experiment_name = get_experiment_name(naming_config)\n",
        "checkpoint_dir = Path(f\"checkpoints/{data_mode}\") / experiment_name\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "4SbhurkIyPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'model_config': model_config,\n",
        "    'training_config': training_config\n",
        "}\n",
        "save_config(config, checkpoint_dir)"
      ],
      "metadata": {
        "id": "xH6YZo5GyPEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = training_config['batch_size']\n",
        "training_set = DataLoader(OneToOne(\"training\"), batch_size=batch_size, shuffle=True)\n",
        "testing_set  = DataLoader(OneToOne(\"validation\"), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model with device\n",
        "model = GatedFNO1d(**{k: v for k, v in model_config.items()})"
      ],
      "metadata": {
        "id": "f8R7ABMEyPCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, training_history = train_model(\n",
        "    model=model,\n",
        "    training_set=training_set,\n",
        "    testing_set=testing_set,\n",
        "    config=training_config,\n",
        "    checkpoint_dir=checkpoint_dir\n",
        ")\n",
        "\n",
        "print(f\"Training completed. Gated Model saved in {checkpoint_dir}\")\n",
        "print(f\"Best validation loss: {training_history['best_val_loss']:.6f} at epoch {training_history['best_epoch']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kip1MRTWHovj",
        "outputId": "a5db10ca-ff91-42c0-eefa-30591940e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.031874, Validation Loss: 0.034051\n",
            "Epoch: 1, Train Loss: 0.031276, Validation Loss: 0.033389\n",
            "Epoch: 2, Train Loss: 0.030836, Validation Loss: 0.032500\n",
            "Epoch: 3, Train Loss: 0.030179, Validation Loss: 0.031341\n",
            "Epoch: 4, Train Loss: 0.029100, Validation Loss: 0.030345\n",
            "Epoch: 5, Train Loss: 0.027499, Validation Loss: 0.027364\n",
            "Epoch: 6, Train Loss: 0.025127, Validation Loss: 0.024558\n",
            "Epoch: 7, Train Loss: 0.022599, Validation Loss: 0.021741\n",
            "Epoch: 8, Train Loss: 0.020725, Validation Loss: 0.020447\n",
            "Epoch: 9, Train Loss: 0.019442, Validation Loss: 0.019337\n",
            "Epoch: 10, Train Loss: 0.017974, Validation Loss: 0.018054\n",
            "Epoch: 11, Train Loss: 0.016637, Validation Loss: 0.016942\n",
            "Epoch: 12, Train Loss: 0.015496, Validation Loss: 0.016228\n",
            "Epoch: 13, Train Loss: 0.014721, Validation Loss: 0.015775\n",
            "Epoch: 14, Train Loss: 0.014548, Validation Loss: 0.015653\n",
            "Epoch: 15, Train Loss: 0.014387, Validation Loss: 0.015244\n",
            "Epoch: 16, Train Loss: 0.014219, Validation Loss: 0.015172\n",
            "Epoch: 17, Train Loss: 0.013945, Validation Loss: 0.014881\n",
            "Epoch: 18, Train Loss: 0.013905, Validation Loss: 0.014943\n",
            "Epoch: 19, Train Loss: 0.013430, Validation Loss: 0.014305\n",
            "Epoch: 20, Train Loss: 0.013084, Validation Loss: 0.013847\n",
            "Epoch: 21, Train Loss: 0.012761, Validation Loss: 0.013343\n",
            "Epoch: 22, Train Loss: 0.012004, Validation Loss: 0.012696\n",
            "Epoch: 23, Train Loss: 0.011349, Validation Loss: 0.012060\n",
            "Epoch: 24, Train Loss: 0.010339, Validation Loss: 0.010430\n",
            "Epoch: 25, Train Loss: 0.009071, Validation Loss: 0.009494\n",
            "Epoch: 26, Train Loss: 0.008136, Validation Loss: 0.008615\n",
            "Epoch: 27, Train Loss: 0.007443, Validation Loss: 0.007912\n",
            "Epoch: 28, Train Loss: 0.006826, Validation Loss: 0.007643\n",
            "Epoch: 29, Train Loss: 0.007154, Validation Loss: 0.007938\n",
            "Epoch: 30, Train Loss: 0.008631, Validation Loss: 0.007422\n",
            "Epoch: 31, Train Loss: 0.006816, Validation Loss: 0.007174\n",
            "Epoch: 32, Train Loss: 0.006067, Validation Loss: 0.006052\n",
            "Epoch: 33, Train Loss: 0.005517, Validation Loss: 0.005550\n",
            "Epoch: 34, Train Loss: 0.005025, Validation Loss: 0.005165\n",
            "Epoch: 35, Train Loss: 0.004763, Validation Loss: 0.005812\n",
            "Epoch: 36, Train Loss: 0.004708, Validation Loss: 0.005564\n",
            "Epoch: 37, Train Loss: 0.004390, Validation Loss: 0.005421\n",
            "Epoch: 38, Train Loss: 0.004406, Validation Loss: 0.004628\n",
            "Epoch: 39, Train Loss: 0.004036, Validation Loss: 0.004045\n",
            "Epoch: 40, Train Loss: 0.003724, Validation Loss: 0.004080\n",
            "Epoch: 41, Train Loss: 0.003602, Validation Loss: 0.003825\n",
            "Epoch: 42, Train Loss: 0.003670, Validation Loss: 0.004158\n",
            "Epoch: 43, Train Loss: 0.003440, Validation Loss: 0.003611\n",
            "Epoch: 44, Train Loss: 0.003231, Validation Loss: 0.003607\n",
            "Epoch: 45, Train Loss: 0.003188, Validation Loss: 0.003560\n",
            "Epoch: 46, Train Loss: 0.003185, Validation Loss: 0.003377\n",
            "Epoch: 47, Train Loss: 0.003161, Validation Loss: 0.003340\n",
            "Epoch: 48, Train Loss: 0.003132, Validation Loss: 0.003583\n",
            "Epoch: 49, Train Loss: 0.003562, Validation Loss: 0.004298\n",
            "Epoch: 50, Train Loss: 0.003242, Validation Loss: 0.003519\n",
            "Epoch: 51, Train Loss: 0.002938, Validation Loss: 0.003394\n",
            "Epoch: 52, Train Loss: 0.003094, Validation Loss: 0.003611\n",
            "Epoch: 53, Train Loss: 0.003048, Validation Loss: 0.003516\n",
            "Epoch: 54, Train Loss: 0.003027, Validation Loss: 0.003128\n",
            "Epoch: 55, Train Loss: 0.002852, Validation Loss: 0.003320\n",
            "Epoch: 56, Train Loss: 0.002722, Validation Loss: 0.003136\n",
            "Epoch: 57, Train Loss: 0.002831, Validation Loss: 0.003080\n",
            "Epoch: 58, Train Loss: 0.003022, Validation Loss: 0.003134\n",
            "Epoch: 59, Train Loss: 0.002958, Validation Loss: 0.003152\n",
            "Epoch: 60, Train Loss: 0.002834, Validation Loss: 0.003196\n",
            "Epoch: 61, Train Loss: 0.002766, Validation Loss: 0.003226\n",
            "Epoch: 62, Train Loss: 0.002775, Validation Loss: 0.003630\n",
            "Epoch: 63, Train Loss: 0.002653, Validation Loss: 0.002982\n",
            "Epoch: 64, Train Loss: 0.002778, Validation Loss: 0.003207\n",
            "Epoch: 65, Train Loss: 0.002825, Validation Loss: 0.003114\n",
            "Epoch: 66, Train Loss: 0.002878, Validation Loss: 0.003025\n",
            "Epoch: 67, Train Loss: 0.002818, Validation Loss: 0.003444\n",
            "Epoch: 68, Train Loss: 0.002790, Validation Loss: 0.003342\n",
            "Epoch: 69, Train Loss: 0.002684, Validation Loss: 0.003470\n",
            "Epoch: 70, Train Loss: 0.002525, Validation Loss: 0.002931\n",
            "Epoch: 71, Train Loss: 0.002505, Validation Loss: 0.002817\n",
            "Epoch: 72, Train Loss: 0.002449, Validation Loss: 0.002824\n",
            "Epoch: 73, Train Loss: 0.002438, Validation Loss: 0.002986\n",
            "Epoch: 74, Train Loss: 0.002397, Validation Loss: 0.002816\n",
            "Epoch: 75, Train Loss: 0.002460, Validation Loss: 0.002893\n",
            "Epoch: 76, Train Loss: 0.002511, Validation Loss: 0.002946\n",
            "Epoch: 77, Train Loss: 0.002747, Validation Loss: 0.002865\n",
            "Epoch: 78, Train Loss: 0.002584, Validation Loss: 0.002934\n",
            "Epoch: 79, Train Loss: 0.002390, Validation Loss: 0.002773\n",
            "Epoch: 80, Train Loss: 0.002478, Validation Loss: 0.002790\n",
            "Epoch: 81, Train Loss: 0.002259, Validation Loss: 0.002676\n",
            "Epoch: 82, Train Loss: 0.002272, Validation Loss: 0.002728\n",
            "Epoch: 83, Train Loss: 0.002483, Validation Loss: 0.002982\n",
            "Epoch: 84, Train Loss: 0.002473, Validation Loss: 0.003174\n",
            "Epoch: 85, Train Loss: 0.002519, Validation Loss: 0.003091\n",
            "Epoch: 86, Train Loss: 0.002588, Validation Loss: 0.002828\n",
            "Epoch: 87, Train Loss: 0.002379, Validation Loss: 0.002646\n",
            "Epoch: 88, Train Loss: 0.002229, Validation Loss: 0.002494\n",
            "Epoch: 89, Train Loss: 0.002545, Validation Loss: 0.002592\n",
            "Epoch: 90, Train Loss: 0.002319, Validation Loss: 0.002641\n",
            "Epoch: 91, Train Loss: 0.002226, Validation Loss: 0.002579\n",
            "Epoch: 92, Train Loss: 0.002064, Validation Loss: 0.002639\n",
            "Epoch: 93, Train Loss: 0.002096, Validation Loss: 0.003039\n",
            "Epoch: 94, Train Loss: 0.002279, Validation Loss: 0.002852\n",
            "Epoch: 95, Train Loss: 0.002106, Validation Loss: 0.002459\n",
            "Epoch: 96, Train Loss: 0.002045, Validation Loss: 0.002572\n",
            "Epoch: 97, Train Loss: 0.002110, Validation Loss: 0.002876\n",
            "Epoch: 98, Train Loss: 0.002339, Validation Loss: 0.003061\n",
            "Epoch: 99, Train Loss: 0.002066, Validation Loss: 0.003137\n",
            "Epoch: 100, Train Loss: 0.002465, Validation Loss: 0.002673\n",
            "Epoch: 101, Train Loss: 0.002016, Validation Loss: 0.002417\n",
            "Epoch: 102, Train Loss: 0.001937, Validation Loss: 0.002553\n",
            "Epoch: 103, Train Loss: 0.002048, Validation Loss: 0.002315\n",
            "Epoch: 104, Train Loss: 0.001890, Validation Loss: 0.002343\n",
            "Epoch: 105, Train Loss: 0.001892, Validation Loss: 0.002475\n",
            "Epoch: 106, Train Loss: 0.001891, Validation Loss: 0.002595\n",
            "Epoch: 107, Train Loss: 0.001839, Validation Loss: 0.002348\n",
            "Epoch: 108, Train Loss: 0.001838, Validation Loss: 0.002236\n",
            "Epoch: 109, Train Loss: 0.001823, Validation Loss: 0.002267\n",
            "Epoch: 110, Train Loss: 0.001854, Validation Loss: 0.002252\n",
            "Epoch: 111, Train Loss: 0.001811, Validation Loss: 0.002297\n",
            "Epoch: 112, Train Loss: 0.001788, Validation Loss: 0.002289\n",
            "Epoch: 113, Train Loss: 0.001810, Validation Loss: 0.002283\n",
            "Epoch: 114, Train Loss: 0.001801, Validation Loss: 0.002280\n",
            "Epoch: 115, Train Loss: 0.001767, Validation Loss: 0.002363\n",
            "Epoch: 116, Train Loss: 0.001732, Validation Loss: 0.002174\n",
            "Epoch: 117, Train Loss: 0.001737, Validation Loss: 0.002258\n",
            "Epoch: 118, Train Loss: 0.001775, Validation Loss: 0.002448\n",
            "Epoch: 119, Train Loss: 0.001792, Validation Loss: 0.002529\n",
            "Epoch: 120, Train Loss: 0.001818, Validation Loss: 0.002136\n",
            "Epoch: 121, Train Loss: 0.001709, Validation Loss: 0.002199\n",
            "Epoch: 122, Train Loss: 0.001746, Validation Loss: 0.002306\n",
            "Epoch: 123, Train Loss: 0.001855, Validation Loss: 0.002497\n",
            "Epoch: 124, Train Loss: 0.001697, Validation Loss: 0.002245\n",
            "Epoch: 125, Train Loss: 0.001670, Validation Loss: 0.002136\n",
            "Epoch: 126, Train Loss: 0.001640, Validation Loss: 0.002124\n",
            "Epoch: 127, Train Loss: 0.001638, Validation Loss: 0.002290\n",
            "Epoch: 128, Train Loss: 0.001661, Validation Loss: 0.002245\n",
            "Epoch: 129, Train Loss: 0.001780, Validation Loss: 0.002237\n",
            "Epoch: 130, Train Loss: 0.001660, Validation Loss: 0.002034\n",
            "Epoch: 131, Train Loss: 0.001657, Validation Loss: 0.002141\n",
            "Epoch: 132, Train Loss: 0.001610, Validation Loss: 0.002270\n",
            "Epoch: 133, Train Loss: 0.001630, Validation Loss: 0.002061\n",
            "Epoch: 134, Train Loss: 0.001625, Validation Loss: 0.002227\n",
            "Epoch: 135, Train Loss: 0.001691, Validation Loss: 0.002108\n",
            "Epoch: 136, Train Loss: 0.001657, Validation Loss: 0.002139\n",
            "Epoch: 137, Train Loss: 0.001633, Validation Loss: 0.002076\n",
            "Epoch: 138, Train Loss: 0.001691, Validation Loss: 0.001978\n",
            "Epoch: 139, Train Loss: 0.001575, Validation Loss: 0.002179\n",
            "Epoch: 140, Train Loss: 0.001577, Validation Loss: 0.001981\n",
            "Epoch: 141, Train Loss: 0.001687, Validation Loss: 0.002151\n",
            "Epoch: 142, Train Loss: 0.001744, Validation Loss: 0.002145\n",
            "Epoch: 143, Train Loss: 0.001685, Validation Loss: 0.002261\n",
            "Epoch: 144, Train Loss: 0.001531, Validation Loss: 0.002016\n",
            "Epoch: 145, Train Loss: 0.001544, Validation Loss: 0.002009\n",
            "Epoch: 146, Train Loss: 0.001586, Validation Loss: 0.002001\n",
            "Epoch: 147, Train Loss: 0.001560, Validation Loss: 0.002062\n",
            "Epoch: 148, Train Loss: 0.001588, Validation Loss: 0.001981\n",
            "Epoch: 149, Train Loss: 0.001521, Validation Loss: 0.001981\n",
            "Epoch: 150, Train Loss: 0.001534, Validation Loss: 0.002053\n",
            "Epoch: 151, Train Loss: 0.001488, Validation Loss: 0.002128\n",
            "Epoch: 152, Train Loss: 0.001476, Validation Loss: 0.001960\n",
            "Epoch: 153, Train Loss: 0.001490, Validation Loss: 0.001944\n",
            "Epoch: 154, Train Loss: 0.001492, Validation Loss: 0.001973\n",
            "Epoch: 155, Train Loss: 0.001482, Validation Loss: 0.001975\n",
            "Epoch: 156, Train Loss: 0.001615, Validation Loss: 0.001980\n",
            "Epoch: 157, Train Loss: 0.001513, Validation Loss: 0.001964\n",
            "Epoch: 158, Train Loss: 0.001530, Validation Loss: 0.002103\n",
            "Epoch: 159, Train Loss: 0.001480, Validation Loss: 0.001946\n",
            "Epoch: 160, Train Loss: 0.001499, Validation Loss: 0.001942\n",
            "Epoch: 161, Train Loss: 0.001441, Validation Loss: 0.001902\n",
            "Epoch: 162, Train Loss: 0.001427, Validation Loss: 0.001910\n",
            "Epoch: 163, Train Loss: 0.001456, Validation Loss: 0.001833\n",
            "Epoch: 164, Train Loss: 0.001472, Validation Loss: 0.002448\n",
            "Epoch: 165, Train Loss: 0.001609, Validation Loss: 0.001865\n",
            "Epoch: 166, Train Loss: 0.001434, Validation Loss: 0.001845\n",
            "Epoch: 167, Train Loss: 0.001433, Validation Loss: 0.001881\n",
            "Epoch: 168, Train Loss: 0.001435, Validation Loss: 0.001848\n",
            "Epoch: 169, Train Loss: 0.001437, Validation Loss: 0.001867\n",
            "Epoch: 170, Train Loss: 0.001417, Validation Loss: 0.001936\n",
            "Epoch: 171, Train Loss: 0.001449, Validation Loss: 0.001806\n",
            "Epoch: 172, Train Loss: 0.001427, Validation Loss: 0.001876\n",
            "Epoch: 173, Train Loss: 0.001368, Validation Loss: 0.001791\n",
            "Epoch: 174, Train Loss: 0.001352, Validation Loss: 0.001853\n",
            "Epoch: 175, Train Loss: 0.001358, Validation Loss: 0.001814\n",
            "Epoch: 176, Train Loss: 0.001482, Validation Loss: 0.002067\n",
            "Epoch: 177, Train Loss: 0.001383, Validation Loss: 0.001787\n",
            "Epoch: 178, Train Loss: 0.001369, Validation Loss: 0.001855\n",
            "Epoch: 179, Train Loss: 0.001291, Validation Loss: 0.001809\n",
            "Epoch: 180, Train Loss: 0.001337, Validation Loss: 0.001775\n",
            "Epoch: 181, Train Loss: 0.001345, Validation Loss: 0.001781\n",
            "Epoch: 182, Train Loss: 0.001375, Validation Loss: 0.001750\n",
            "Epoch: 183, Train Loss: 0.001351, Validation Loss: 0.001755\n",
            "Epoch: 184, Train Loss: 0.001295, Validation Loss: 0.001719\n",
            "Epoch: 185, Train Loss: 0.001463, Validation Loss: 0.001991\n",
            "Epoch: 186, Train Loss: 0.001453, Validation Loss: 0.001735\n",
            "Epoch: 187, Train Loss: 0.001377, Validation Loss: 0.001985\n",
            "Epoch: 188, Train Loss: 0.001390, Validation Loss: 0.001916\n",
            "Epoch: 189, Train Loss: 0.001358, Validation Loss: 0.001890\n",
            "Epoch: 190, Train Loss: 0.001412, Validation Loss: 0.001912\n",
            "Epoch: 191, Train Loss: 0.001428, Validation Loss: 0.001857\n",
            "Epoch: 192, Train Loss: 0.001232, Validation Loss: 0.001654\n",
            "Epoch: 193, Train Loss: 0.001288, Validation Loss: 0.001785\n",
            "Epoch: 194, Train Loss: 0.001252, Validation Loss: 0.001660\n",
            "Epoch: 195, Train Loss: 0.001254, Validation Loss: 0.001648\n",
            "Epoch: 196, Train Loss: 0.001210, Validation Loss: 0.001574\n",
            "Epoch: 197, Train Loss: 0.001221, Validation Loss: 0.001914\n",
            "Epoch: 198, Train Loss: 0.001262, Validation Loss: 0.001722\n",
            "Epoch: 199, Train Loss: 0.001226, Validation Loss: 0.001849\n",
            "Epoch: 200, Train Loss: 0.001231, Validation Loss: 0.001551\n",
            "Epoch: 201, Train Loss: 0.001192, Validation Loss: 0.001584\n",
            "Epoch: 202, Train Loss: 0.001169, Validation Loss: 0.001670\n",
            "Epoch: 203, Train Loss: 0.001196, Validation Loss: 0.001652\n",
            "Epoch: 204, Train Loss: 0.001117, Validation Loss: 0.001538\n",
            "Epoch: 205, Train Loss: 0.001104, Validation Loss: 0.001494\n",
            "Epoch: 206, Train Loss: 0.001124, Validation Loss: 0.001478\n",
            "Epoch: 207, Train Loss: 0.001077, Validation Loss: 0.001525\n",
            "Epoch: 208, Train Loss: 0.001119, Validation Loss: 0.001421\n",
            "Epoch: 209, Train Loss: 0.001060, Validation Loss: 0.001444\n",
            "Epoch: 210, Train Loss: 0.001060, Validation Loss: 0.001406\n",
            "Epoch: 211, Train Loss: 0.001075, Validation Loss: 0.001550\n",
            "Epoch: 212, Train Loss: 0.001103, Validation Loss: 0.001522\n",
            "Epoch: 213, Train Loss: 0.001072, Validation Loss: 0.001451\n",
            "Epoch: 214, Train Loss: 0.001068, Validation Loss: 0.001355\n",
            "Epoch: 215, Train Loss: 0.000978, Validation Loss: 0.001364\n",
            "Epoch: 216, Train Loss: 0.000947, Validation Loss: 0.001340\n",
            "Epoch: 217, Train Loss: 0.000959, Validation Loss: 0.001309\n",
            "Epoch: 218, Train Loss: 0.000981, Validation Loss: 0.001266\n",
            "Epoch: 219, Train Loss: 0.000933, Validation Loss: 0.001322\n",
            "Epoch: 220, Train Loss: 0.000951, Validation Loss: 0.001300\n",
            "Epoch: 221, Train Loss: 0.000976, Validation Loss: 0.001358\n",
            "Epoch: 222, Train Loss: 0.000976, Validation Loss: 0.001459\n",
            "Epoch: 223, Train Loss: 0.000940, Validation Loss: 0.001190\n",
            "Epoch: 224, Train Loss: 0.000865, Validation Loss: 0.001207\n",
            "Epoch: 225, Train Loss: 0.000901, Validation Loss: 0.001237\n",
            "Epoch: 226, Train Loss: 0.000896, Validation Loss: 0.001217\n",
            "Epoch: 227, Train Loss: 0.000897, Validation Loss: 0.001170\n",
            "Epoch: 228, Train Loss: 0.000884, Validation Loss: 0.001142\n",
            "Epoch: 229, Train Loss: 0.000848, Validation Loss: 0.001195\n",
            "Epoch: 230, Train Loss: 0.000801, Validation Loss: 0.001162\n",
            "Epoch: 231, Train Loss: 0.000824, Validation Loss: 0.001134\n",
            "Epoch: 232, Train Loss: 0.000807, Validation Loss: 0.001189\n",
            "Epoch: 233, Train Loss: 0.000808, Validation Loss: 0.001122\n",
            "Epoch: 234, Train Loss: 0.000779, Validation Loss: 0.001053\n",
            "Epoch: 235, Train Loss: 0.000764, Validation Loss: 0.001062\n",
            "Epoch: 236, Train Loss: 0.000776, Validation Loss: 0.001077\n",
            "Epoch: 237, Train Loss: 0.000740, Validation Loss: 0.001104\n",
            "Epoch: 238, Train Loss: 0.000758, Validation Loss: 0.001007\n",
            "Epoch: 239, Train Loss: 0.000727, Validation Loss: 0.001042\n",
            "Epoch: 240, Train Loss: 0.000716, Validation Loss: 0.000975\n",
            "Epoch: 241, Train Loss: 0.000751, Validation Loss: 0.001037\n",
            "Epoch: 242, Train Loss: 0.000795, Validation Loss: 0.000966\n",
            "Epoch: 243, Train Loss: 0.000689, Validation Loss: 0.000950\n",
            "Epoch: 244, Train Loss: 0.000708, Validation Loss: 0.000996\n",
            "Epoch: 245, Train Loss: 0.000729, Validation Loss: 0.000928\n",
            "Epoch: 246, Train Loss: 0.000751, Validation Loss: 0.000965\n",
            "Epoch: 247, Train Loss: 0.000672, Validation Loss: 0.000932\n",
            "Epoch: 248, Train Loss: 0.000663, Validation Loss: 0.000914\n",
            "Epoch: 249, Train Loss: 0.000631, Validation Loss: 0.000893\n",
            "Epoch: 250, Train Loss: 0.000632, Validation Loss: 0.000937\n",
            "Epoch: 251, Train Loss: 0.000618, Validation Loss: 0.000870\n",
            "Epoch: 252, Train Loss: 0.000611, Validation Loss: 0.000907\n",
            "Epoch: 253, Train Loss: 0.000613, Validation Loss: 0.000857\n",
            "Epoch: 254, Train Loss: 0.000598, Validation Loss: 0.000844\n",
            "Epoch: 255, Train Loss: 0.000634, Validation Loss: 0.000884\n",
            "Epoch: 256, Train Loss: 0.000602, Validation Loss: 0.000837\n",
            "Epoch: 257, Train Loss: 0.000588, Validation Loss: 0.000856\n",
            "Epoch: 258, Train Loss: 0.000570, Validation Loss: 0.000801\n",
            "Epoch: 259, Train Loss: 0.000569, Validation Loss: 0.000803\n",
            "Epoch: 260, Train Loss: 0.000567, Validation Loss: 0.000793\n",
            "Epoch: 261, Train Loss: 0.000556, Validation Loss: 0.000817\n",
            "Epoch: 262, Train Loss: 0.000557, Validation Loss: 0.000845\n",
            "Epoch: 263, Train Loss: 0.000554, Validation Loss: 0.000804\n",
            "Epoch: 264, Train Loss: 0.000542, Validation Loss: 0.000732\n",
            "Epoch: 265, Train Loss: 0.000520, Validation Loss: 0.000782\n",
            "Epoch: 266, Train Loss: 0.000541, Validation Loss: 0.000742\n",
            "Epoch: 267, Train Loss: 0.000551, Validation Loss: 0.000787\n",
            "Epoch: 268, Train Loss: 0.000534, Validation Loss: 0.000747\n",
            "Epoch: 269, Train Loss: 0.000501, Validation Loss: 0.000749\n",
            "Epoch: 270, Train Loss: 0.000535, Validation Loss: 0.000746\n",
            "Epoch: 271, Train Loss: 0.000512, Validation Loss: 0.000751\n",
            "Epoch: 272, Train Loss: 0.000493, Validation Loss: 0.000687\n",
            "Epoch: 273, Train Loss: 0.000523, Validation Loss: 0.000749\n",
            "Epoch: 274, Train Loss: 0.000544, Validation Loss: 0.000807\n",
            "Epoch: 275, Train Loss: 0.000525, Validation Loss: 0.000737\n",
            "Epoch: 276, Train Loss: 0.000496, Validation Loss: 0.000667\n",
            "Epoch: 277, Train Loss: 0.000485, Validation Loss: 0.000702\n",
            "Epoch: 278, Train Loss: 0.000489, Validation Loss: 0.000679\n",
            "Epoch: 279, Train Loss: 0.000489, Validation Loss: 0.000716\n",
            "Epoch: 280, Train Loss: 0.000486, Validation Loss: 0.000671\n",
            "Epoch: 281, Train Loss: 0.000477, Validation Loss: 0.000657\n",
            "Epoch: 282, Train Loss: 0.000474, Validation Loss: 0.000700\n",
            "Epoch: 283, Train Loss: 0.000483, Validation Loss: 0.000656\n",
            "Epoch: 284, Train Loss: 0.000466, Validation Loss: 0.000682\n",
            "Epoch: 285, Train Loss: 0.000478, Validation Loss: 0.000650\n",
            "Epoch: 286, Train Loss: 0.000460, Validation Loss: 0.000639\n",
            "Epoch: 287, Train Loss: 0.000432, Validation Loss: 0.000655\n",
            "Epoch: 288, Train Loss: 0.000450, Validation Loss: 0.000640\n",
            "Epoch: 289, Train Loss: 0.000441, Validation Loss: 0.000629\n",
            "Epoch: 290, Train Loss: 0.000456, Validation Loss: 0.000669\n",
            "Epoch: 291, Train Loss: 0.000450, Validation Loss: 0.000652\n",
            "Epoch: 292, Train Loss: 0.000474, Validation Loss: 0.000665\n",
            "Epoch: 293, Train Loss: 0.000461, Validation Loss: 0.000628\n",
            "Epoch: 294, Train Loss: 0.000436, Validation Loss: 0.000650\n",
            "Epoch: 295, Train Loss: 0.000420, Validation Loss: 0.000629\n",
            "Epoch: 296, Train Loss: 0.000427, Validation Loss: 0.000642\n",
            "Epoch: 297, Train Loss: 0.000428, Validation Loss: 0.000604\n",
            "Epoch: 298, Train Loss: 0.000414, Validation Loss: 0.000577\n",
            "Epoch: 299, Train Loss: 0.000405, Validation Loss: 0.000703\n",
            "Epoch: 300, Train Loss: 0.000416, Validation Loss: 0.000592\n",
            "Epoch: 301, Train Loss: 0.000375, Validation Loss: 0.000563\n",
            "Epoch: 302, Train Loss: 0.000373, Validation Loss: 0.000567\n",
            "Epoch: 303, Train Loss: 0.000372, Validation Loss: 0.000567\n",
            "Epoch: 304, Train Loss: 0.000370, Validation Loss: 0.000563\n",
            "Epoch: 305, Train Loss: 0.000367, Validation Loss: 0.000566\n",
            "Epoch: 306, Train Loss: 0.000369, Validation Loss: 0.000567\n",
            "Epoch: 307, Train Loss: 0.000370, Validation Loss: 0.000564\n",
            "Epoch: 308, Train Loss: 0.000370, Validation Loss: 0.000564\n",
            "Epoch: 309, Train Loss: 0.000371, Validation Loss: 0.000568\n",
            "Epoch: 310, Train Loss: 0.000368, Validation Loss: 0.000566\n",
            "Epoch: 311, Train Loss: 0.000367, Validation Loss: 0.000561\n",
            "Epoch: 312, Train Loss: 0.000368, Validation Loss: 0.000563\n",
            "Epoch: 313, Train Loss: 0.000367, Validation Loss: 0.000557\n",
            "Epoch: 314, Train Loss: 0.000368, Validation Loss: 0.000564\n",
            "Epoch: 315, Train Loss: 0.000365, Validation Loss: 0.000565\n",
            "Epoch: 316, Train Loss: 0.000368, Validation Loss: 0.000557\n",
            "Epoch: 317, Train Loss: 0.000368, Validation Loss: 0.000568\n",
            "Epoch: 318, Train Loss: 0.000365, Validation Loss: 0.000559\n",
            "Epoch: 319, Train Loss: 0.000367, Validation Loss: 0.000556\n",
            "Epoch: 320, Train Loss: 0.000368, Validation Loss: 0.000561\n",
            "Epoch: 321, Train Loss: 0.000366, Validation Loss: 0.000562\n",
            "Epoch: 322, Train Loss: 0.000366, Validation Loss: 0.000562\n",
            "Epoch: 323, Train Loss: 0.000365, Validation Loss: 0.000555\n",
            "Epoch: 324, Train Loss: 0.000364, Validation Loss: 0.000555\n",
            "Epoch: 325, Train Loss: 0.000366, Validation Loss: 0.000563\n",
            "Epoch: 326, Train Loss: 0.000365, Validation Loss: 0.000556\n",
            "Epoch: 327, Train Loss: 0.000364, Validation Loss: 0.000557\n",
            "Epoch: 328, Train Loss: 0.000366, Validation Loss: 0.000556\n",
            "Epoch: 329, Train Loss: 0.000362, Validation Loss: 0.000558\n",
            "Epoch: 330, Train Loss: 0.000366, Validation Loss: 0.000565\n",
            "Epoch: 331, Train Loss: 0.000365, Validation Loss: 0.000551\n",
            "Epoch: 332, Train Loss: 0.000361, Validation Loss: 0.000557\n",
            "Epoch: 333, Train Loss: 0.000364, Validation Loss: 0.000556\n",
            "Epoch: 334, Train Loss: 0.000364, Validation Loss: 0.000552\n",
            "Epoch: 335, Train Loss: 0.000364, Validation Loss: 0.000558\n",
            "Epoch: 336, Train Loss: 0.000362, Validation Loss: 0.000557\n",
            "Epoch: 337, Train Loss: 0.000362, Validation Loss: 0.000553\n",
            "Epoch: 338, Train Loss: 0.000365, Validation Loss: 0.000554\n",
            "Epoch: 339, Train Loss: 0.000361, Validation Loss: 0.000550\n",
            "Epoch: 340, Train Loss: 0.000361, Validation Loss: 0.000551\n",
            "Epoch: 341, Train Loss: 0.000363, Validation Loss: 0.000554\n",
            "Epoch: 342, Train Loss: 0.000360, Validation Loss: 0.000546\n",
            "Epoch: 343, Train Loss: 0.000360, Validation Loss: 0.000549\n",
            "Epoch: 344, Train Loss: 0.000360, Validation Loss: 0.000553\n",
            "Epoch: 345, Train Loss: 0.000362, Validation Loss: 0.000549\n",
            "Epoch: 346, Train Loss: 0.000359, Validation Loss: 0.000553\n",
            "Epoch: 347, Train Loss: 0.000362, Validation Loss: 0.000547\n",
            "Epoch: 348, Train Loss: 0.000358, Validation Loss: 0.000553\n",
            "Epoch: 349, Train Loss: 0.000359, Validation Loss: 0.000548\n",
            "Epoch: 350, Train Loss: 0.000365, Validation Loss: 0.000555\n",
            "Epoch: 351, Train Loss: 0.000360, Validation Loss: 0.000550\n",
            "Epoch: 352, Train Loss: 0.000360, Validation Loss: 0.000545\n",
            "Epoch: 353, Train Loss: 0.000357, Validation Loss: 0.000546\n",
            "Epoch: 354, Train Loss: 0.000357, Validation Loss: 0.000544\n",
            "Epoch: 355, Train Loss: 0.000361, Validation Loss: 0.000552\n",
            "Epoch: 356, Train Loss: 0.000359, Validation Loss: 0.000544\n",
            "Epoch: 357, Train Loss: 0.000358, Validation Loss: 0.000558\n",
            "Epoch: 358, Train Loss: 0.000362, Validation Loss: 0.000546\n",
            "Epoch: 359, Train Loss: 0.000358, Validation Loss: 0.000544\n",
            "Epoch: 360, Train Loss: 0.000357, Validation Loss: 0.000545\n",
            "Epoch: 361, Train Loss: 0.000357, Validation Loss: 0.000544\n",
            "Epoch: 362, Train Loss: 0.000358, Validation Loss: 0.000547\n",
            "Epoch: 363, Train Loss: 0.000354, Validation Loss: 0.000542\n",
            "Epoch: 364, Train Loss: 0.000356, Validation Loss: 0.000543\n",
            "Epoch: 365, Train Loss: 0.000354, Validation Loss: 0.000542\n",
            "Epoch: 366, Train Loss: 0.000358, Validation Loss: 0.000541\n",
            "Epoch: 367, Train Loss: 0.000363, Validation Loss: 0.000549\n",
            "Epoch: 368, Train Loss: 0.000359, Validation Loss: 0.000541\n",
            "Epoch: 369, Train Loss: 0.000357, Validation Loss: 0.000546\n",
            "Epoch: 370, Train Loss: 0.000356, Validation Loss: 0.000536\n",
            "Epoch: 371, Train Loss: 0.000351, Validation Loss: 0.000538\n",
            "Epoch: 372, Train Loss: 0.000353, Validation Loss: 0.000543\n",
            "Epoch: 373, Train Loss: 0.000353, Validation Loss: 0.000536\n",
            "Epoch: 374, Train Loss: 0.000354, Validation Loss: 0.000546\n",
            "Epoch: 375, Train Loss: 0.000355, Validation Loss: 0.000535\n",
            "Epoch: 376, Train Loss: 0.000353, Validation Loss: 0.000539\n",
            "Epoch: 377, Train Loss: 0.000352, Validation Loss: 0.000535\n",
            "Epoch: 378, Train Loss: 0.000351, Validation Loss: 0.000538\n",
            "Epoch: 379, Train Loss: 0.000352, Validation Loss: 0.000538\n",
            "Epoch: 380, Train Loss: 0.000351, Validation Loss: 0.000539\n",
            "Epoch: 381, Train Loss: 0.000350, Validation Loss: 0.000541\n",
            "Epoch: 382, Train Loss: 0.000350, Validation Loss: 0.000531\n",
            "Epoch: 383, Train Loss: 0.000352, Validation Loss: 0.000537\n",
            "Epoch: 384, Train Loss: 0.000353, Validation Loss: 0.000530\n",
            "Epoch: 385, Train Loss: 0.000351, Validation Loss: 0.000534\n",
            "Epoch: 386, Train Loss: 0.000351, Validation Loss: 0.000534\n",
            "Epoch: 387, Train Loss: 0.000354, Validation Loss: 0.000539\n",
            "Epoch: 388, Train Loss: 0.000350, Validation Loss: 0.000531\n",
            "Epoch: 389, Train Loss: 0.000350, Validation Loss: 0.000537\n",
            "Epoch: 390, Train Loss: 0.000349, Validation Loss: 0.000532\n",
            "Epoch: 391, Train Loss: 0.000346, Validation Loss: 0.000532\n",
            "Epoch: 392, Train Loss: 0.000349, Validation Loss: 0.000530\n",
            "Epoch: 393, Train Loss: 0.000350, Validation Loss: 0.000534\n",
            "Epoch: 394, Train Loss: 0.000346, Validation Loss: 0.000533\n",
            "Epoch: 395, Train Loss: 0.000347, Validation Loss: 0.000534\n",
            "Epoch: 396, Train Loss: 0.000347, Validation Loss: 0.000529\n",
            "Epoch: 397, Train Loss: 0.000348, Validation Loss: 0.000534\n",
            "Epoch: 398, Train Loss: 0.000347, Validation Loss: 0.000531\n",
            "Epoch: 399, Train Loss: 0.000348, Validation Loss: 0.000533\n",
            "Epoch: 400, Train Loss: 0.000344, Validation Loss: 0.000526\n",
            "Epoch: 401, Train Loss: 0.000348, Validation Loss: 0.000524\n",
            "Epoch: 402, Train Loss: 0.000345, Validation Loss: 0.000530\n",
            "Epoch: 403, Train Loss: 0.000346, Validation Loss: 0.000522\n",
            "Epoch: 404, Train Loss: 0.000347, Validation Loss: 0.000530\n",
            "Epoch: 405, Train Loss: 0.000346, Validation Loss: 0.000530\n",
            "Epoch: 406, Train Loss: 0.000350, Validation Loss: 0.000533\n",
            "Epoch: 407, Train Loss: 0.000346, Validation Loss: 0.000520\n",
            "Epoch: 408, Train Loss: 0.000348, Validation Loss: 0.000528\n",
            "Epoch: 409, Train Loss: 0.000344, Validation Loss: 0.000528\n",
            "Epoch: 410, Train Loss: 0.000343, Validation Loss: 0.000528\n",
            "Epoch: 411, Train Loss: 0.000346, Validation Loss: 0.000526\n",
            "Epoch: 412, Train Loss: 0.000346, Validation Loss: 0.000535\n",
            "Epoch: 413, Train Loss: 0.000347, Validation Loss: 0.000518\n",
            "Epoch: 414, Train Loss: 0.000345, Validation Loss: 0.000523\n",
            "Epoch: 415, Train Loss: 0.000342, Validation Loss: 0.000531\n",
            "Epoch: 416, Train Loss: 0.000346, Validation Loss: 0.000526\n",
            "Epoch: 417, Train Loss: 0.000341, Validation Loss: 0.000522\n",
            "Epoch: 418, Train Loss: 0.000343, Validation Loss: 0.000519\n",
            "Epoch: 419, Train Loss: 0.000341, Validation Loss: 0.000520\n",
            "Epoch: 420, Train Loss: 0.000342, Validation Loss: 0.000518\n",
            "Epoch: 421, Train Loss: 0.000344, Validation Loss: 0.000521\n",
            "Epoch: 422, Train Loss: 0.000343, Validation Loss: 0.000524\n",
            "Epoch: 423, Train Loss: 0.000346, Validation Loss: 0.000523\n",
            "Epoch: 424, Train Loss: 0.000339, Validation Loss: 0.000528\n",
            "Epoch: 425, Train Loss: 0.000340, Validation Loss: 0.000513\n",
            "Epoch: 426, Train Loss: 0.000339, Validation Loss: 0.000522\n",
            "Epoch: 427, Train Loss: 0.000338, Validation Loss: 0.000523\n",
            "Epoch: 428, Train Loss: 0.000339, Validation Loss: 0.000514\n",
            "Epoch: 429, Train Loss: 0.000337, Validation Loss: 0.000523\n",
            "Epoch: 430, Train Loss: 0.000342, Validation Loss: 0.000517\n",
            "Epoch: 431, Train Loss: 0.000342, Validation Loss: 0.000517\n",
            "Epoch: 432, Train Loss: 0.000341, Validation Loss: 0.000520\n",
            "Epoch: 433, Train Loss: 0.000336, Validation Loss: 0.000514\n",
            "Epoch: 434, Train Loss: 0.000337, Validation Loss: 0.000522\n",
            "Epoch: 435, Train Loss: 0.000341, Validation Loss: 0.000526\n",
            "Epoch: 436, Train Loss: 0.000337, Validation Loss: 0.000517\n",
            "Epoch: 437, Train Loss: 0.000334, Validation Loss: 0.000514\n",
            "Epoch: 438, Train Loss: 0.000334, Validation Loss: 0.000516\n",
            "Epoch: 439, Train Loss: 0.000337, Validation Loss: 0.000514\n",
            "Epoch: 440, Train Loss: 0.000338, Validation Loss: 0.000511\n",
            "Epoch: 441, Train Loss: 0.000337, Validation Loss: 0.000522\n",
            "Epoch: 442, Train Loss: 0.000335, Validation Loss: 0.000514\n",
            "Epoch: 443, Train Loss: 0.000339, Validation Loss: 0.000510\n",
            "Epoch: 444, Train Loss: 0.000335, Validation Loss: 0.000508\n",
            "Epoch: 445, Train Loss: 0.000338, Validation Loss: 0.000515\n",
            "Epoch: 446, Train Loss: 0.000334, Validation Loss: 0.000506\n",
            "Epoch: 447, Train Loss: 0.000336, Validation Loss: 0.000519\n",
            "Epoch: 448, Train Loss: 0.000336, Validation Loss: 0.000513\n",
            "Epoch: 449, Train Loss: 0.000335, Validation Loss: 0.000508\n",
            "Epoch: 450, Train Loss: 0.000333, Validation Loss: 0.000512\n",
            "Epoch: 451, Train Loss: 0.000335, Validation Loss: 0.000510\n",
            "Epoch: 452, Train Loss: 0.000335, Validation Loss: 0.000510\n",
            "Epoch: 453, Train Loss: 0.000335, Validation Loss: 0.000510\n",
            "Epoch: 454, Train Loss: 0.000337, Validation Loss: 0.000515\n",
            "Epoch: 455, Train Loss: 0.000332, Validation Loss: 0.000505\n",
            "Epoch: 456, Train Loss: 0.000335, Validation Loss: 0.000507\n",
            "Epoch: 457, Train Loss: 0.000333, Validation Loss: 0.000513\n",
            "Epoch: 458, Train Loss: 0.000332, Validation Loss: 0.000504\n",
            "Epoch: 459, Train Loss: 0.000333, Validation Loss: 0.000509\n",
            "Epoch: 460, Train Loss: 0.000332, Validation Loss: 0.000508\n",
            "Epoch: 461, Train Loss: 0.000333, Validation Loss: 0.000506\n",
            "Epoch: 462, Train Loss: 0.000336, Validation Loss: 0.000508\n",
            "Epoch: 463, Train Loss: 0.000330, Validation Loss: 0.000499\n",
            "Epoch: 464, Train Loss: 0.000331, Validation Loss: 0.000504\n",
            "Epoch: 465, Train Loss: 0.000334, Validation Loss: 0.000515\n",
            "Epoch: 466, Train Loss: 0.000330, Validation Loss: 0.000501\n",
            "Epoch: 467, Train Loss: 0.000330, Validation Loss: 0.000504\n",
            "Epoch: 468, Train Loss: 0.000329, Validation Loss: 0.000508\n",
            "Epoch: 469, Train Loss: 0.000328, Validation Loss: 0.000503\n",
            "Epoch: 470, Train Loss: 0.000331, Validation Loss: 0.000506\n",
            "Epoch: 471, Train Loss: 0.000330, Validation Loss: 0.000500\n",
            "Epoch: 472, Train Loss: 0.000330, Validation Loss: 0.000508\n",
            "Epoch: 473, Train Loss: 0.000329, Validation Loss: 0.000509\n",
            "Epoch: 474, Train Loss: 0.000327, Validation Loss: 0.000499\n",
            "Epoch: 475, Train Loss: 0.000329, Validation Loss: 0.000506\n",
            "Epoch: 476, Train Loss: 0.000331, Validation Loss: 0.000502\n",
            "Epoch: 477, Train Loss: 0.000325, Validation Loss: 0.000501\n",
            "Epoch: 478, Train Loss: 0.000329, Validation Loss: 0.000496\n",
            "Epoch: 479, Train Loss: 0.000329, Validation Loss: 0.000505\n",
            "Epoch: 480, Train Loss: 0.000327, Validation Loss: 0.000501\n",
            "Epoch: 481, Train Loss: 0.000330, Validation Loss: 0.000507\n",
            "Epoch: 482, Train Loss: 0.000331, Validation Loss: 0.000511\n",
            "Epoch: 483, Train Loss: 0.000328, Validation Loss: 0.000511\n",
            "Epoch: 484, Train Loss: 0.000329, Validation Loss: 0.000497\n",
            "Epoch: 485, Train Loss: 0.000328, Validation Loss: 0.000503\n",
            "Epoch: 486, Train Loss: 0.000326, Validation Loss: 0.000504\n",
            "Epoch: 487, Train Loss: 0.000327, Validation Loss: 0.000505\n",
            "Epoch: 488, Train Loss: 0.000328, Validation Loss: 0.000496\n",
            "Epoch: 489, Train Loss: 0.000325, Validation Loss: 0.000496\n",
            "Epoch: 490, Train Loss: 0.000325, Validation Loss: 0.000500\n",
            "Epoch: 491, Train Loss: 0.000325, Validation Loss: 0.000496\n",
            "Epoch: 492, Train Loss: 0.000325, Validation Loss: 0.000495\n",
            "Epoch: 493, Train Loss: 0.000325, Validation Loss: 0.000496\n",
            "Epoch: 494, Train Loss: 0.000326, Validation Loss: 0.000496\n",
            "Epoch: 495, Train Loss: 0.000327, Validation Loss: 0.000492\n",
            "Epoch: 496, Train Loss: 0.000323, Validation Loss: 0.000493\n",
            "Epoch: 497, Train Loss: 0.000323, Validation Loss: 0.000494\n",
            "Epoch: 498, Train Loss: 0.000328, Validation Loss: 0.000494\n",
            "Epoch: 499, Train Loss: 0.000325, Validation Loss: 0.000496\n",
            "Epoch: 500, Train Loss: 0.000324, Validation Loss: 0.000504\n",
            "Epoch: 501, Train Loss: 0.000325, Validation Loss: 0.000491\n",
            "Epoch: 502, Train Loss: 0.000325, Validation Loss: 0.000486\n",
            "Epoch: 503, Train Loss: 0.000326, Validation Loss: 0.000495\n",
            "Epoch: 504, Train Loss: 0.000322, Validation Loss: 0.000492\n",
            "Epoch: 505, Train Loss: 0.000323, Validation Loss: 0.000487\n",
            "Epoch: 506, Train Loss: 0.000321, Validation Loss: 0.000488\n",
            "Epoch: 507, Train Loss: 0.000319, Validation Loss: 0.000488\n",
            "Epoch: 508, Train Loss: 0.000324, Validation Loss: 0.000490\n",
            "Epoch: 509, Train Loss: 0.000321, Validation Loss: 0.000490\n",
            "Epoch: 510, Train Loss: 0.000325, Validation Loss: 0.000486\n",
            "Epoch: 511, Train Loss: 0.000321, Validation Loss: 0.000493\n",
            "Epoch: 512, Train Loss: 0.000320, Validation Loss: 0.000490\n",
            "Epoch: 513, Train Loss: 0.000323, Validation Loss: 0.000492\n",
            "Epoch: 514, Train Loss: 0.000320, Validation Loss: 0.000491\n",
            "Epoch: 515, Train Loss: 0.000319, Validation Loss: 0.000486\n",
            "Epoch: 516, Train Loss: 0.000320, Validation Loss: 0.000488\n",
            "Epoch: 517, Train Loss: 0.000324, Validation Loss: 0.000488\n",
            "Epoch: 518, Train Loss: 0.000317, Validation Loss: 0.000485\n",
            "Epoch: 519, Train Loss: 0.000321, Validation Loss: 0.000493\n",
            "Epoch: 520, Train Loss: 0.000319, Validation Loss: 0.000499\n",
            "Epoch: 521, Train Loss: 0.000319, Validation Loss: 0.000493\n",
            "Epoch: 522, Train Loss: 0.000320, Validation Loss: 0.000486\n",
            "Epoch: 523, Train Loss: 0.000319, Validation Loss: 0.000485\n",
            "Epoch: 524, Train Loss: 0.000319, Validation Loss: 0.000483\n",
            "Epoch: 525, Train Loss: 0.000319, Validation Loss: 0.000486\n",
            "Epoch: 526, Train Loss: 0.000318, Validation Loss: 0.000489\n",
            "Epoch: 527, Train Loss: 0.000316, Validation Loss: 0.000483\n",
            "Epoch: 528, Train Loss: 0.000317, Validation Loss: 0.000485\n",
            "Epoch: 529, Train Loss: 0.000318, Validation Loss: 0.000487\n",
            "Epoch: 530, Train Loss: 0.000317, Validation Loss: 0.000477\n",
            "Epoch: 531, Train Loss: 0.000320, Validation Loss: 0.000488\n",
            "Epoch: 532, Train Loss: 0.000316, Validation Loss: 0.000483\n",
            "Epoch: 533, Train Loss: 0.000315, Validation Loss: 0.000483\n",
            "Epoch: 534, Train Loss: 0.000314, Validation Loss: 0.000482\n",
            "Epoch: 535, Train Loss: 0.000315, Validation Loss: 0.000478\n",
            "Epoch: 536, Train Loss: 0.000318, Validation Loss: 0.000486\n",
            "Epoch: 537, Train Loss: 0.000316, Validation Loss: 0.000486\n",
            "Epoch: 538, Train Loss: 0.000316, Validation Loss: 0.000490\n",
            "Epoch: 539, Train Loss: 0.000314, Validation Loss: 0.000486\n",
            "Epoch: 540, Train Loss: 0.000315, Validation Loss: 0.000484\n",
            "Epoch: 541, Train Loss: 0.000314, Validation Loss: 0.000478\n",
            "Epoch: 542, Train Loss: 0.000314, Validation Loss: 0.000479\n",
            "Epoch: 543, Train Loss: 0.000319, Validation Loss: 0.000486\n",
            "Epoch: 544, Train Loss: 0.000315, Validation Loss: 0.000474\n",
            "Epoch: 545, Train Loss: 0.000316, Validation Loss: 0.000484\n",
            "Epoch: 546, Train Loss: 0.000313, Validation Loss: 0.000475\n",
            "Epoch: 547, Train Loss: 0.000312, Validation Loss: 0.000483\n",
            "Epoch: 548, Train Loss: 0.000313, Validation Loss: 0.000469\n",
            "Epoch: 549, Train Loss: 0.000318, Validation Loss: 0.000483\n",
            "Epoch: 550, Train Loss: 0.000318, Validation Loss: 0.000475\n",
            "Epoch: 551, Train Loss: 0.000313, Validation Loss: 0.000474\n",
            "Epoch: 552, Train Loss: 0.000313, Validation Loss: 0.000479\n",
            "Epoch: 553, Train Loss: 0.000310, Validation Loss: 0.000482\n",
            "Epoch: 554, Train Loss: 0.000310, Validation Loss: 0.000478\n",
            "Epoch: 555, Train Loss: 0.000310, Validation Loss: 0.000480\n",
            "Epoch: 556, Train Loss: 0.000310, Validation Loss: 0.000473\n",
            "Epoch: 557, Train Loss: 0.000308, Validation Loss: 0.000484\n",
            "Epoch: 558, Train Loss: 0.000310, Validation Loss: 0.000475\n",
            "Epoch: 559, Train Loss: 0.000306, Validation Loss: 0.000471\n",
            "Epoch: 560, Train Loss: 0.000310, Validation Loss: 0.000476\n",
            "Epoch: 561, Train Loss: 0.000310, Validation Loss: 0.000477\n",
            "Epoch: 562, Train Loss: 0.000310, Validation Loss: 0.000472\n",
            "Epoch: 563, Train Loss: 0.000311, Validation Loss: 0.000474\n",
            "Epoch: 564, Train Loss: 0.000308, Validation Loss: 0.000469\n",
            "Epoch: 565, Train Loss: 0.000308, Validation Loss: 0.000475\n",
            "Epoch: 566, Train Loss: 0.000310, Validation Loss: 0.000472\n",
            "Epoch: 567, Train Loss: 0.000307, Validation Loss: 0.000471\n",
            "Epoch: 568, Train Loss: 0.000308, Validation Loss: 0.000469\n",
            "Epoch: 569, Train Loss: 0.000307, Validation Loss: 0.000474\n",
            "Epoch: 570, Train Loss: 0.000310, Validation Loss: 0.000475\n",
            "Epoch: 571, Train Loss: 0.000310, Validation Loss: 0.000471\n",
            "Epoch: 572, Train Loss: 0.000307, Validation Loss: 0.000476\n",
            "Epoch: 573, Train Loss: 0.000308, Validation Loss: 0.000489\n",
            "Epoch: 574, Train Loss: 0.000313, Validation Loss: 0.000473\n",
            "Epoch: 575, Train Loss: 0.000308, Validation Loss: 0.000467\n",
            "Epoch: 576, Train Loss: 0.000309, Validation Loss: 0.000472\n",
            "Epoch: 577, Train Loss: 0.000310, Validation Loss: 0.000472\n",
            "Epoch: 578, Train Loss: 0.000305, Validation Loss: 0.000473\n",
            "Epoch: 579, Train Loss: 0.000311, Validation Loss: 0.000467\n",
            "Epoch: 580, Train Loss: 0.000308, Validation Loss: 0.000467\n",
            "Epoch: 581, Train Loss: 0.000304, Validation Loss: 0.000470\n",
            "Epoch: 582, Train Loss: 0.000305, Validation Loss: 0.000462\n",
            "Epoch: 583, Train Loss: 0.000303, Validation Loss: 0.000464\n",
            "Epoch: 584, Train Loss: 0.000305, Validation Loss: 0.000473\n",
            "Epoch: 585, Train Loss: 0.000305, Validation Loss: 0.000461\n",
            "Epoch: 586, Train Loss: 0.000305, Validation Loss: 0.000467\n",
            "Epoch: 587, Train Loss: 0.000304, Validation Loss: 0.000461\n",
            "Epoch: 588, Train Loss: 0.000303, Validation Loss: 0.000465\n",
            "Epoch: 589, Train Loss: 0.000306, Validation Loss: 0.000460\n",
            "Epoch: 590, Train Loss: 0.000306, Validation Loss: 0.000470\n",
            "Epoch: 591, Train Loss: 0.000305, Validation Loss: 0.000463\n",
            "Epoch: 592, Train Loss: 0.000304, Validation Loss: 0.000463\n",
            "Epoch: 593, Train Loss: 0.000302, Validation Loss: 0.000463\n",
            "Epoch: 594, Train Loss: 0.000305, Validation Loss: 0.000458\n",
            "Epoch: 595, Train Loss: 0.000303, Validation Loss: 0.000469\n",
            "Epoch: 596, Train Loss: 0.000306, Validation Loss: 0.000461\n",
            "Epoch: 597, Train Loss: 0.000305, Validation Loss: 0.000469\n",
            "Epoch: 598, Train Loss: 0.000307, Validation Loss: 0.000456\n",
            "Epoch: 599, Train Loss: 0.000304, Validation Loss: 0.000468\n",
            "Epoch: 600, Train Loss: 0.000299, Validation Loss: 0.000463\n",
            "Epoch: 601, Train Loss: 0.000296, Validation Loss: 0.000461\n",
            "Epoch: 602, Train Loss: 0.000297, Validation Loss: 0.000461\n",
            "Epoch: 603, Train Loss: 0.000297, Validation Loss: 0.000460\n",
            "Epoch: 604, Train Loss: 0.000296, Validation Loss: 0.000460\n",
            "Epoch: 605, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 606, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 607, Train Loss: 0.000296, Validation Loss: 0.000461\n",
            "Epoch: 608, Train Loss: 0.000296, Validation Loss: 0.000461\n",
            "Epoch: 609, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 610, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 611, Train Loss: 0.000295, Validation Loss: 0.000461\n",
            "Epoch: 612, Train Loss: 0.000297, Validation Loss: 0.000461\n",
            "Epoch: 613, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 614, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 615, Train Loss: 0.000294, Validation Loss: 0.000461\n",
            "Epoch: 616, Train Loss: 0.000296, Validation Loss: 0.000459\n",
            "Epoch: 617, Train Loss: 0.000295, Validation Loss: 0.000461\n",
            "Epoch: 618, Train Loss: 0.000297, Validation Loss: 0.000461\n",
            "Epoch: 619, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 620, Train Loss: 0.000296, Validation Loss: 0.000460\n",
            "Epoch: 621, Train Loss: 0.000296, Validation Loss: 0.000461\n",
            "Epoch: 622, Train Loss: 0.000296, Validation Loss: 0.000460\n",
            "Epoch: 623, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 624, Train Loss: 0.000296, Validation Loss: 0.000461\n",
            "Epoch: 625, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 626, Train Loss: 0.000296, Validation Loss: 0.000460\n",
            "Epoch: 627, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 628, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 629, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 630, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 631, Train Loss: 0.000295, Validation Loss: 0.000461\n",
            "Epoch: 632, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 633, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 634, Train Loss: 0.000297, Validation Loss: 0.000460\n",
            "Epoch: 635, Train Loss: 0.000295, Validation Loss: 0.000461\n",
            "Epoch: 636, Train Loss: 0.000296, Validation Loss: 0.000459\n",
            "Epoch: 637, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 638, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 639, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 640, Train Loss: 0.000296, Validation Loss: 0.000459\n",
            "Epoch: 641, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 642, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 643, Train Loss: 0.000293, Validation Loss: 0.000460\n",
            "Epoch: 644, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 645, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 646, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 647, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 648, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 649, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 650, Train Loss: 0.000293, Validation Loss: 0.000459\n",
            "Epoch: 651, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 652, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 653, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 654, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 655, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 656, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 657, Train Loss: 0.000295, Validation Loss: 0.000458\n",
            "Epoch: 658, Train Loss: 0.000293, Validation Loss: 0.000460\n",
            "Epoch: 659, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 660, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 661, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 662, Train Loss: 0.000294, Validation Loss: 0.000458\n",
            "Epoch: 663, Train Loss: 0.000293, Validation Loss: 0.000459\n",
            "Epoch: 664, Train Loss: 0.000295, Validation Loss: 0.000458\n",
            "Epoch: 665, Train Loss: 0.000294, Validation Loss: 0.000460\n",
            "Epoch: 666, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 667, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 668, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 669, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 670, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 671, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Epoch: 672, Train Loss: 0.000293, Validation Loss: 0.000459\n",
            "Epoch: 673, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 674, Train Loss: 0.000294, Validation Loss: 0.000459\n",
            "Epoch: 675, Train Loss: 0.000296, Validation Loss: 0.000460\n",
            "Epoch: 676, Train Loss: 0.000295, Validation Loss: 0.000460\n",
            "Epoch: 677, Train Loss: 0.000295, Validation Loss: 0.000459\n",
            "Early stopping triggered at epoch 678\n",
            "Training completed. Gated Model saved in checkpoints/onetoone/fno_m30_w16_d2_lr0.0005_20250411_185139\n",
            "Best validation loss: 0.000456 at epoch 598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, training_history = train_model(\n",
        "    model=model,\n",
        "    training_set=training_set,\n",
        "    testing_set=testing_set,\n",
        "    config=training_config,\n",
        "    checkpoint_dir=checkpoint_dir\n",
        ")\n",
        "\n",
        "print(f\"Training completed. Model saved in {checkpoint_dir}\")\n",
        "print(f\"Best validation loss: {training_history['best_val_loss']:.6f} at epoch {training_history['best_epoch']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtJkk7qLzcSR",
        "outputId": "38f02862-307c-46e6-8055-7f1dd70ea740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 0.031413, Validation Loss: 0.031788\n",
            "Epoch: 1, Train Loss: 0.028967, Validation Loss: 0.028255\n",
            "Epoch: 2, Train Loss: 0.025593, Validation Loss: 0.023365\n",
            "Epoch: 3, Train Loss: 0.021810, Validation Loss: 0.020979\n",
            "Epoch: 4, Train Loss: 0.019611, Validation Loss: 0.018133\n",
            "Epoch: 5, Train Loss: 0.017027, Validation Loss: 0.016073\n",
            "Epoch: 6, Train Loss: 0.014855, Validation Loss: 0.014894\n",
            "Epoch: 7, Train Loss: 0.013317, Validation Loss: 0.013690\n",
            "Epoch: 8, Train Loss: 0.012136, Validation Loss: 0.012008\n",
            "Epoch: 9, Train Loss: 0.010304, Validation Loss: 0.009497\n",
            "Epoch: 10, Train Loss: 0.007944, Validation Loss: 0.006859\n",
            "Epoch: 11, Train Loss: 0.005328, Validation Loss: 0.004613\n",
            "Epoch: 12, Train Loss: 0.003498, Validation Loss: 0.003184\n",
            "Epoch: 13, Train Loss: 0.002638, Validation Loss: 0.002899\n",
            "Epoch: 14, Train Loss: 0.002585, Validation Loss: 0.003017\n",
            "Epoch: 15, Train Loss: 0.002447, Validation Loss: 0.002326\n",
            "Epoch: 16, Train Loss: 0.002077, Validation Loss: 0.002447\n",
            "Epoch: 17, Train Loss: 0.002024, Validation Loss: 0.002565\n",
            "Epoch: 18, Train Loss: 0.001982, Validation Loss: 0.002145\n",
            "Epoch: 19, Train Loss: 0.001867, Validation Loss: 0.002340\n",
            "Epoch: 20, Train Loss: 0.001798, Validation Loss: 0.002043\n",
            "Epoch: 21, Train Loss: 0.001753, Validation Loss: 0.002083\n",
            "Epoch: 22, Train Loss: 0.001635, Validation Loss: 0.002008\n",
            "Epoch: 23, Train Loss: 0.001673, Validation Loss: 0.002109\n",
            "Epoch: 24, Train Loss: 0.001565, Validation Loss: 0.001862\n",
            "Epoch: 25, Train Loss: 0.001531, Validation Loss: 0.001907\n",
            "Epoch: 26, Train Loss: 0.001489, Validation Loss: 0.001882\n",
            "Epoch: 27, Train Loss: 0.001497, Validation Loss: 0.001752\n",
            "Epoch: 28, Train Loss: 0.001490, Validation Loss: 0.001754\n",
            "Epoch: 29, Train Loss: 0.001477, Validation Loss: 0.001765\n",
            "Epoch: 30, Train Loss: 0.001395, Validation Loss: 0.001697\n",
            "Epoch: 31, Train Loss: 0.001318, Validation Loss: 0.001865\n",
            "Epoch: 32, Train Loss: 0.001304, Validation Loss: 0.001677\n",
            "Epoch: 33, Train Loss: 0.001278, Validation Loss: 0.001886\n",
            "Epoch: 34, Train Loss: 0.001243, Validation Loss: 0.001508\n",
            "Epoch: 35, Train Loss: 0.001179, Validation Loss: 0.001648\n",
            "Epoch: 36, Train Loss: 0.001214, Validation Loss: 0.001546\n",
            "Epoch: 37, Train Loss: 0.001130, Validation Loss: 0.001384\n",
            "Epoch: 38, Train Loss: 0.001065, Validation Loss: 0.001505\n",
            "Epoch: 39, Train Loss: 0.001090, Validation Loss: 0.001574\n",
            "Epoch: 40, Train Loss: 0.001063, Validation Loss: 0.001362\n",
            "Epoch: 41, Train Loss: 0.001014, Validation Loss: 0.001485\n",
            "Epoch: 42, Train Loss: 0.000951, Validation Loss: 0.001333\n",
            "Epoch: 43, Train Loss: 0.000976, Validation Loss: 0.001325\n",
            "Epoch: 44, Train Loss: 0.000883, Validation Loss: 0.001203\n",
            "Epoch: 45, Train Loss: 0.000843, Validation Loss: 0.001279\n",
            "Epoch: 46, Train Loss: 0.000875, Validation Loss: 0.001280\n",
            "Epoch: 47, Train Loss: 0.000930, Validation Loss: 0.001350\n",
            "Epoch: 48, Train Loss: 0.000810, Validation Loss: 0.001108\n",
            "Epoch: 49, Train Loss: 0.000781, Validation Loss: 0.001114\n",
            "Epoch: 50, Train Loss: 0.000781, Validation Loss: 0.001134\n",
            "Epoch: 51, Train Loss: 0.000776, Validation Loss: 0.001154\n",
            "Epoch: 52, Train Loss: 0.000740, Validation Loss: 0.001083\n",
            "Epoch: 53, Train Loss: 0.000717, Validation Loss: 0.001100\n",
            "Epoch: 54, Train Loss: 0.000734, Validation Loss: 0.001084\n",
            "Epoch: 55, Train Loss: 0.000724, Validation Loss: 0.000970\n",
            "Epoch: 56, Train Loss: 0.000687, Validation Loss: 0.001193\n",
            "Epoch: 57, Train Loss: 0.000689, Validation Loss: 0.001039\n",
            "Epoch: 58, Train Loss: 0.000705, Validation Loss: 0.001080\n",
            "Epoch: 59, Train Loss: 0.000720, Validation Loss: 0.000986\n",
            "Epoch: 60, Train Loss: 0.000729, Validation Loss: 0.000985\n",
            "Epoch: 61, Train Loss: 0.000695, Validation Loss: 0.000875\n",
            "Epoch: 62, Train Loss: 0.000646, Validation Loss: 0.000891\n",
            "Epoch: 63, Train Loss: 0.000666, Validation Loss: 0.000959\n",
            "Epoch: 64, Train Loss: 0.000636, Validation Loss: 0.000882\n",
            "Epoch: 65, Train Loss: 0.000598, Validation Loss: 0.000914\n",
            "Epoch: 66, Train Loss: 0.000586, Validation Loss: 0.000889\n",
            "Epoch: 67, Train Loss: 0.000592, Validation Loss: 0.000930\n",
            "Epoch: 68, Train Loss: 0.000572, Validation Loss: 0.000940\n",
            "Epoch: 69, Train Loss: 0.000569, Validation Loss: 0.000919\n",
            "Epoch: 70, Train Loss: 0.000563, Validation Loss: 0.000976\n",
            "Epoch: 71, Train Loss: 0.000544, Validation Loss: 0.000989\n",
            "Epoch: 72, Train Loss: 0.000547, Validation Loss: 0.000793\n",
            "Epoch: 73, Train Loss: 0.000515, Validation Loss: 0.000772\n",
            "Epoch: 74, Train Loss: 0.000491, Validation Loss: 0.000772\n",
            "Epoch: 75, Train Loss: 0.000494, Validation Loss: 0.000857\n",
            "Epoch: 76, Train Loss: 0.000466, Validation Loss: 0.000707\n",
            "Epoch: 77, Train Loss: 0.000474, Validation Loss: 0.000771\n",
            "Epoch: 78, Train Loss: 0.000533, Validation Loss: 0.000768\n",
            "Epoch: 79, Train Loss: 0.000464, Validation Loss: 0.000647\n",
            "Epoch: 80, Train Loss: 0.000392, Validation Loss: 0.000620\n",
            "Epoch: 81, Train Loss: 0.000379, Validation Loss: 0.000604\n",
            "Epoch: 82, Train Loss: 0.000368, Validation Loss: 0.000584\n",
            "Epoch: 83, Train Loss: 0.000367, Validation Loss: 0.000630\n",
            "Epoch: 84, Train Loss: 0.000384, Validation Loss: 0.000594\n",
            "Epoch: 85, Train Loss: 0.000348, Validation Loss: 0.000587\n",
            "Epoch: 86, Train Loss: 0.000345, Validation Loss: 0.000582\n",
            "Epoch: 87, Train Loss: 0.000391, Validation Loss: 0.000656\n",
            "Epoch: 88, Train Loss: 0.000369, Validation Loss: 0.000513\n",
            "Epoch: 89, Train Loss: 0.000351, Validation Loss: 0.000524\n",
            "Epoch: 90, Train Loss: 0.000310, Validation Loss: 0.000501\n",
            "Epoch: 91, Train Loss: 0.000306, Validation Loss: 0.000556\n",
            "Epoch: 92, Train Loss: 0.000304, Validation Loss: 0.000476\n",
            "Epoch: 93, Train Loss: 0.000322, Validation Loss: 0.000524\n",
            "Epoch: 94, Train Loss: 0.000311, Validation Loss: 0.000453\n",
            "Epoch: 95, Train Loss: 0.000267, Validation Loss: 0.000476\n",
            "Epoch: 96, Train Loss: 0.000305, Validation Loss: 0.000453\n",
            "Epoch: 97, Train Loss: 0.000273, Validation Loss: 0.000503\n",
            "Epoch: 98, Train Loss: 0.000317, Validation Loss: 0.000569\n",
            "Epoch: 99, Train Loss: 0.000286, Validation Loss: 0.000474\n",
            "Epoch: 100, Train Loss: 0.000295, Validation Loss: 0.000457\n",
            "Epoch: 101, Train Loss: 0.000257, Validation Loss: 0.000390\n",
            "Epoch: 102, Train Loss: 0.000268, Validation Loss: 0.000393\n",
            "Epoch: 103, Train Loss: 0.000235, Validation Loss: 0.000367\n",
            "Epoch: 104, Train Loss: 0.000213, Validation Loss: 0.000367\n",
            "Epoch: 105, Train Loss: 0.000217, Validation Loss: 0.000436\n",
            "Epoch: 106, Train Loss: 0.000234, Validation Loss: 0.000395\n",
            "Epoch: 107, Train Loss: 0.000243, Validation Loss: 0.000361\n",
            "Epoch: 108, Train Loss: 0.000200, Validation Loss: 0.000351\n",
            "Epoch: 109, Train Loss: 0.000199, Validation Loss: 0.000368\n",
            "Epoch: 110, Train Loss: 0.000209, Validation Loss: 0.000355\n",
            "Epoch: 111, Train Loss: 0.000227, Validation Loss: 0.000373\n",
            "Epoch: 112, Train Loss: 0.000211, Validation Loss: 0.000378\n",
            "Epoch: 113, Train Loss: 0.000210, Validation Loss: 0.000409\n",
            "Epoch: 114, Train Loss: 0.000208, Validation Loss: 0.000346\n",
            "Epoch: 115, Train Loss: 0.000177, Validation Loss: 0.000323\n",
            "Epoch: 116, Train Loss: 0.000179, Validation Loss: 0.000324\n",
            "Epoch: 117, Train Loss: 0.000181, Validation Loss: 0.000305\n",
            "Epoch: 118, Train Loss: 0.000170, Validation Loss: 0.000312\n",
            "Epoch: 119, Train Loss: 0.000160, Validation Loss: 0.000293\n",
            "Epoch: 120, Train Loss: 0.000169, Validation Loss: 0.000289\n",
            "Epoch: 121, Train Loss: 0.000168, Validation Loss: 0.000276\n",
            "Epoch: 122, Train Loss: 0.000164, Validation Loss: 0.000285\n",
            "Epoch: 123, Train Loss: 0.000201, Validation Loss: 0.000355\n",
            "Epoch: 124, Train Loss: 0.000237, Validation Loss: 0.000329\n",
            "Epoch: 125, Train Loss: 0.000197, Validation Loss: 0.000352\n",
            "Epoch: 126, Train Loss: 0.000167, Validation Loss: 0.000318\n",
            "Epoch: 127, Train Loss: 0.000175, Validation Loss: 0.000281\n",
            "Epoch: 128, Train Loss: 0.000149, Validation Loss: 0.000296\n",
            "Epoch: 129, Train Loss: 0.000153, Validation Loss: 0.000290\n",
            "Epoch: 130, Train Loss: 0.000152, Validation Loss: 0.000267\n",
            "Epoch: 131, Train Loss: 0.000146, Validation Loss: 0.000271\n",
            "Epoch: 132, Train Loss: 0.000162, Validation Loss: 0.000262\n",
            "Epoch: 133, Train Loss: 0.000143, Validation Loss: 0.000272\n",
            "Epoch: 134, Train Loss: 0.000142, Validation Loss: 0.000257\n",
            "Epoch: 135, Train Loss: 0.000143, Validation Loss: 0.000282\n",
            "Epoch: 136, Train Loss: 0.000165, Validation Loss: 0.000270\n",
            "Epoch: 137, Train Loss: 0.000167, Validation Loss: 0.000282\n",
            "Epoch: 138, Train Loss: 0.000222, Validation Loss: 0.000335\n",
            "Epoch: 139, Train Loss: 0.000166, Validation Loss: 0.000242\n",
            "Epoch: 140, Train Loss: 0.000132, Validation Loss: 0.000252\n",
            "Epoch: 141, Train Loss: 0.000145, Validation Loss: 0.000279\n",
            "Epoch: 142, Train Loss: 0.000166, Validation Loss: 0.000263\n",
            "Epoch: 143, Train Loss: 0.000176, Validation Loss: 0.000254\n",
            "Epoch: 144, Train Loss: 0.000142, Validation Loss: 0.000253\n",
            "Epoch: 145, Train Loss: 0.000125, Validation Loss: 0.000243\n",
            "Epoch: 146, Train Loss: 0.000120, Validation Loss: 0.000292\n",
            "Epoch: 147, Train Loss: 0.000168, Validation Loss: 0.000262\n",
            "Epoch: 148, Train Loss: 0.000136, Validation Loss: 0.000275\n",
            "Epoch: 149, Train Loss: 0.000170, Validation Loss: 0.000266\n",
            "Epoch: 150, Train Loss: 0.000139, Validation Loss: 0.000234\n",
            "Epoch: 151, Train Loss: 0.000121, Validation Loss: 0.000251\n",
            "Epoch: 152, Train Loss: 0.000125, Validation Loss: 0.000230\n",
            "Epoch: 153, Train Loss: 0.000114, Validation Loss: 0.000230\n",
            "Epoch: 154, Train Loss: 0.000114, Validation Loss: 0.000228\n",
            "Epoch: 155, Train Loss: 0.000115, Validation Loss: 0.000261\n",
            "Epoch: 156, Train Loss: 0.000135, Validation Loss: 0.000266\n",
            "Epoch: 157, Train Loss: 0.000154, Validation Loss: 0.000332\n",
            "Epoch: 158, Train Loss: 0.000167, Validation Loss: 0.000236\n",
            "Epoch: 159, Train Loss: 0.000148, Validation Loss: 0.000270\n",
            "Epoch: 160, Train Loss: 0.000141, Validation Loss: 0.000263\n",
            "Epoch: 161, Train Loss: 0.000144, Validation Loss: 0.000243\n",
            "Epoch: 162, Train Loss: 0.000161, Validation Loss: 0.000292\n",
            "Epoch: 163, Train Loss: 0.000141, Validation Loss: 0.000275\n",
            "Epoch: 164, Train Loss: 0.000141, Validation Loss: 0.000261\n",
            "Epoch: 165, Train Loss: 0.000133, Validation Loss: 0.000219\n",
            "Epoch: 166, Train Loss: 0.000119, Validation Loss: 0.000213\n",
            "Epoch: 167, Train Loss: 0.000114, Validation Loss: 0.000238\n",
            "Epoch: 168, Train Loss: 0.000123, Validation Loss: 0.000227\n",
            "Epoch: 169, Train Loss: 0.000126, Validation Loss: 0.000223\n",
            "Epoch: 170, Train Loss: 0.000111, Validation Loss: 0.000247\n",
            "Epoch: 171, Train Loss: 0.000117, Validation Loss: 0.000220\n",
            "Epoch: 172, Train Loss: 0.000114, Validation Loss: 0.000221\n",
            "Epoch: 173, Train Loss: 0.000119, Validation Loss: 0.000215\n",
            "Epoch: 174, Train Loss: 0.000106, Validation Loss: 0.000227\n",
            "Epoch: 175, Train Loss: 0.000124, Validation Loss: 0.000250\n",
            "Epoch: 176, Train Loss: 0.000152, Validation Loss: 0.000209\n",
            "Epoch: 177, Train Loss: 0.000117, Validation Loss: 0.000235\n",
            "Epoch: 178, Train Loss: 0.000112, Validation Loss: 0.000209\n",
            "Epoch: 179, Train Loss: 0.000121, Validation Loss: 0.000269\n",
            "Epoch: 180, Train Loss: 0.000169, Validation Loss: 0.000275\n",
            "Epoch: 181, Train Loss: 0.000286, Validation Loss: 0.000480\n",
            "Epoch: 182, Train Loss: 0.000363, Validation Loss: 0.000445\n",
            "Epoch: 183, Train Loss: 0.000296, Validation Loss: 0.000227\n",
            "Epoch: 184, Train Loss: 0.000203, Validation Loss: 0.000235\n",
            "Epoch: 185, Train Loss: 0.000154, Validation Loss: 0.000212\n",
            "Epoch: 186, Train Loss: 0.000123, Validation Loss: 0.000234\n",
            "Epoch: 187, Train Loss: 0.000133, Validation Loss: 0.000198\n",
            "Epoch: 188, Train Loss: 0.000111, Validation Loss: 0.000195\n",
            "Epoch: 189, Train Loss: 0.000099, Validation Loss: 0.000191\n",
            "Epoch: 190, Train Loss: 0.000093, Validation Loss: 0.000187\n",
            "Epoch: 191, Train Loss: 0.000095, Validation Loss: 0.000207\n",
            "Epoch: 192, Train Loss: 0.000094, Validation Loss: 0.000197\n",
            "Epoch: 193, Train Loss: 0.000092, Validation Loss: 0.000187\n",
            "Epoch: 194, Train Loss: 0.000096, Validation Loss: 0.000187\n",
            "Epoch: 195, Train Loss: 0.000099, Validation Loss: 0.000185\n",
            "Epoch: 196, Train Loss: 0.000096, Validation Loss: 0.000194\n",
            "Epoch: 197, Train Loss: 0.000096, Validation Loss: 0.000182\n",
            "Epoch: 198, Train Loss: 0.000085, Validation Loss: 0.000202\n",
            "Epoch: 199, Train Loss: 0.000090, Validation Loss: 0.000185\n",
            "Epoch: 200, Train Loss: 0.000099, Validation Loss: 0.000172\n",
            "Epoch: 201, Train Loss: 0.000100, Validation Loss: 0.000205\n",
            "Epoch: 202, Train Loss: 0.000103, Validation Loss: 0.000183\n",
            "Epoch: 203, Train Loss: 0.000103, Validation Loss: 0.000181\n",
            "Epoch: 204, Train Loss: 0.000103, Validation Loss: 0.000209\n",
            "Epoch: 205, Train Loss: 0.000104, Validation Loss: 0.000191\n",
            "Epoch: 206, Train Loss: 0.000083, Validation Loss: 0.000171\n",
            "Epoch: 207, Train Loss: 0.000086, Validation Loss: 0.000190\n",
            "Epoch: 208, Train Loss: 0.000088, Validation Loss: 0.000190\n",
            "Epoch: 209, Train Loss: 0.000083, Validation Loss: 0.000174\n",
            "Epoch: 210, Train Loss: 0.000080, Validation Loss: 0.000173\n",
            "Epoch: 211, Train Loss: 0.000083, Validation Loss: 0.000180\n",
            "Epoch: 212, Train Loss: 0.000084, Validation Loss: 0.000173\n",
            "Epoch: 213, Train Loss: 0.000081, Validation Loss: 0.000171\n",
            "Epoch: 214, Train Loss: 0.000080, Validation Loss: 0.000194\n",
            "Epoch: 215, Train Loss: 0.000085, Validation Loss: 0.000176\n",
            "Epoch: 216, Train Loss: 0.000081, Validation Loss: 0.000159\n",
            "Epoch: 217, Train Loss: 0.000079, Validation Loss: 0.000176\n",
            "Epoch: 218, Train Loss: 0.000080, Validation Loss: 0.000168\n",
            "Epoch: 219, Train Loss: 0.000079, Validation Loss: 0.000161\n",
            "Epoch: 220, Train Loss: 0.000081, Validation Loss: 0.000201\n",
            "Epoch: 221, Train Loss: 0.000079, Validation Loss: 0.000153\n",
            "Epoch: 222, Train Loss: 0.000075, Validation Loss: 0.000185\n",
            "Epoch: 223, Train Loss: 0.000086, Validation Loss: 0.000167\n",
            "Epoch: 224, Train Loss: 0.000096, Validation Loss: 0.000195\n",
            "Epoch: 225, Train Loss: 0.000084, Validation Loss: 0.000172\n",
            "Epoch: 226, Train Loss: 0.000080, Validation Loss: 0.000162\n",
            "Epoch: 227, Train Loss: 0.000076, Validation Loss: 0.000168\n",
            "Epoch: 228, Train Loss: 0.000075, Validation Loss: 0.000159\n",
            "Epoch: 229, Train Loss: 0.000073, Validation Loss: 0.000162\n",
            "Epoch: 230, Train Loss: 0.000079, Validation Loss: 0.000174\n",
            "Epoch: 231, Train Loss: 0.000082, Validation Loss: 0.000151\n",
            "Epoch: 232, Train Loss: 0.000077, Validation Loss: 0.000158\n",
            "Epoch: 233, Train Loss: 0.000079, Validation Loss: 0.000204\n",
            "Epoch: 234, Train Loss: 0.000092, Validation Loss: 0.000161\n",
            "Epoch: 235, Train Loss: 0.000080, Validation Loss: 0.000165\n",
            "Epoch: 236, Train Loss: 0.000078, Validation Loss: 0.000174\n",
            "Epoch: 237, Train Loss: 0.000095, Validation Loss: 0.000183\n",
            "Epoch: 238, Train Loss: 0.000098, Validation Loss: 0.000158\n",
            "Epoch: 239, Train Loss: 0.000080, Validation Loss: 0.000165\n",
            "Epoch: 240, Train Loss: 0.000073, Validation Loss: 0.000171\n",
            "Epoch: 241, Train Loss: 0.000081, Validation Loss: 0.000152\n",
            "Epoch: 242, Train Loss: 0.000076, Validation Loss: 0.000158\n",
            "Epoch: 243, Train Loss: 0.000070, Validation Loss: 0.000159\n",
            "Epoch: 244, Train Loss: 0.000078, Validation Loss: 0.000156\n",
            "Epoch: 245, Train Loss: 0.000094, Validation Loss: 0.000197\n",
            "Epoch: 246, Train Loss: 0.000105, Validation Loss: 0.000191\n",
            "Epoch: 247, Train Loss: 0.000106, Validation Loss: 0.000177\n",
            "Epoch: 248, Train Loss: 0.000079, Validation Loss: 0.000175\n",
            "Epoch: 249, Train Loss: 0.000086, Validation Loss: 0.000211\n",
            "Epoch: 250, Train Loss: 0.000132, Validation Loss: 0.000210\n",
            "Epoch: 251, Train Loss: 0.000112, Validation Loss: 0.000217\n",
            "Epoch: 252, Train Loss: 0.000101, Validation Loss: 0.000163\n",
            "Epoch: 253, Train Loss: 0.000090, Validation Loss: 0.000179\n",
            "Epoch: 254, Train Loss: 0.000102, Validation Loss: 0.000161\n",
            "Epoch: 255, Train Loss: 0.000078, Validation Loss: 0.000162\n",
            "Epoch: 256, Train Loss: 0.000067, Validation Loss: 0.000141\n",
            "Epoch: 257, Train Loss: 0.000064, Validation Loss: 0.000141\n",
            "Epoch: 258, Train Loss: 0.000064, Validation Loss: 0.000135\n",
            "Epoch: 259, Train Loss: 0.000075, Validation Loss: 0.000173\n",
            "Epoch: 260, Train Loss: 0.000070, Validation Loss: 0.000132\n",
            "Epoch: 261, Train Loss: 0.000060, Validation Loss: 0.000140\n",
            "Epoch: 262, Train Loss: 0.000062, Validation Loss: 0.000132\n",
            "Epoch: 263, Train Loss: 0.000068, Validation Loss: 0.000140\n",
            "Epoch: 264, Train Loss: 0.000083, Validation Loss: 0.000171\n",
            "Epoch: 265, Train Loss: 0.000073, Validation Loss: 0.000145\n",
            "Epoch: 266, Train Loss: 0.000065, Validation Loss: 0.000149\n",
            "Epoch: 267, Train Loss: 0.000062, Validation Loss: 0.000130\n",
            "Epoch: 268, Train Loss: 0.000065, Validation Loss: 0.000148\n",
            "Epoch: 269, Train Loss: 0.000071, Validation Loss: 0.000148\n",
            "Epoch: 270, Train Loss: 0.000070, Validation Loss: 0.000135\n",
            "Epoch: 271, Train Loss: 0.000070, Validation Loss: 0.000144\n",
            "Epoch: 272, Train Loss: 0.000068, Validation Loss: 0.000131\n",
            "Epoch: 273, Train Loss: 0.000063, Validation Loss: 0.000139\n",
            "Epoch: 274, Train Loss: 0.000057, Validation Loss: 0.000127\n",
            "Epoch: 275, Train Loss: 0.000060, Validation Loss: 0.000154\n",
            "Epoch: 276, Train Loss: 0.000068, Validation Loss: 0.000164\n",
            "Epoch: 277, Train Loss: 0.000071, Validation Loss: 0.000150\n",
            "Epoch: 278, Train Loss: 0.000065, Validation Loss: 0.000122\n",
            "Epoch: 279, Train Loss: 0.000065, Validation Loss: 0.000142\n",
            "Epoch: 280, Train Loss: 0.000063, Validation Loss: 0.000136\n",
            "Epoch: 281, Train Loss: 0.000064, Validation Loss: 0.000147\n",
            "Epoch: 282, Train Loss: 0.000099, Validation Loss: 0.000215\n",
            "Epoch: 283, Train Loss: 0.000152, Validation Loss: 0.000268\n",
            "Epoch: 284, Train Loss: 0.000149, Validation Loss: 0.000209\n",
            "Epoch: 285, Train Loss: 0.000130, Validation Loss: 0.000236\n",
            "Epoch: 286, Train Loss: 0.000163, Validation Loss: 0.000264\n",
            "Epoch: 287, Train Loss: 0.000183, Validation Loss: 0.000195\n",
            "Epoch: 288, Train Loss: 0.000136, Validation Loss: 0.000186\n",
            "Epoch: 289, Train Loss: 0.000112, Validation Loss: 0.000178\n",
            "Epoch: 290, Train Loss: 0.000081, Validation Loss: 0.000135\n",
            "Epoch: 291, Train Loss: 0.000066, Validation Loss: 0.000125\n",
            "Epoch: 292, Train Loss: 0.000058, Validation Loss: 0.000120\n",
            "Epoch: 293, Train Loss: 0.000052, Validation Loss: 0.000113\n",
            "Epoch: 294, Train Loss: 0.000052, Validation Loss: 0.000133\n",
            "Epoch: 295, Train Loss: 0.000060, Validation Loss: 0.000121\n",
            "Epoch: 296, Train Loss: 0.000053, Validation Loss: 0.000113\n",
            "Epoch: 297, Train Loss: 0.000049, Validation Loss: 0.000121\n",
            "Epoch: 298, Train Loss: 0.000051, Validation Loss: 0.000120\n",
            "Epoch: 299, Train Loss: 0.000058, Validation Loss: 0.000123\n",
            "Epoch: 300, Train Loss: 0.000045, Validation Loss: 0.000104\n",
            "Epoch: 301, Train Loss: 0.000038, Validation Loss: 0.000104\n",
            "Epoch: 302, Train Loss: 0.000036, Validation Loss: 0.000102\n",
            "Epoch: 303, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 304, Train Loss: 0.000035, Validation Loss: 0.000103\n",
            "Epoch: 305, Train Loss: 0.000035, Validation Loss: 0.000102\n",
            "Epoch: 306, Train Loss: 0.000035, Validation Loss: 0.000102\n",
            "Epoch: 307, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 308, Train Loss: 0.000035, Validation Loss: 0.000103\n",
            "Epoch: 309, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 310, Train Loss: 0.000034, Validation Loss: 0.000102\n",
            "Epoch: 311, Train Loss: 0.000035, Validation Loss: 0.000102\n",
            "Epoch: 312, Train Loss: 0.000034, Validation Loss: 0.000101\n",
            "Epoch: 313, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 314, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 315, Train Loss: 0.000035, Validation Loss: 0.000102\n",
            "Epoch: 316, Train Loss: 0.000035, Validation Loss: 0.000102\n",
            "Epoch: 317, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 318, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 319, Train Loss: 0.000035, Validation Loss: 0.000105\n",
            "Epoch: 320, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 321, Train Loss: 0.000035, Validation Loss: 0.000101\n",
            "Epoch: 322, Train Loss: 0.000034, Validation Loss: 0.000101\n",
            "Epoch: 323, Train Loss: 0.000034, Validation Loss: 0.000101\n",
            "Epoch: 324, Train Loss: 0.000035, Validation Loss: 0.000099\n",
            "Epoch: 325, Train Loss: 0.000035, Validation Loss: 0.000100\n",
            "Epoch: 326, Train Loss: 0.000035, Validation Loss: 0.000099\n",
            "Epoch: 327, Train Loss: 0.000035, Validation Loss: 0.000100\n",
            "Epoch: 328, Train Loss: 0.000035, Validation Loss: 0.000100\n",
            "Epoch: 329, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 330, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 331, Train Loss: 0.000034, Validation Loss: 0.000101\n",
            "Epoch: 332, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 333, Train Loss: 0.000034, Validation Loss: 0.000101\n",
            "Epoch: 334, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 335, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 336, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 337, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 338, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 339, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 340, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 341, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 342, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 343, Train Loss: 0.000035, Validation Loss: 0.000100\n",
            "Epoch: 344, Train Loss: 0.000035, Validation Loss: 0.000098\n",
            "Epoch: 345, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 346, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 347, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 348, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 349, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 350, Train Loss: 0.000034, Validation Loss: 0.000100\n",
            "Epoch: 351, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 352, Train Loss: 0.000033, Validation Loss: 0.000099\n",
            "Epoch: 353, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 354, Train Loss: 0.000034, Validation Loss: 0.000099\n",
            "Epoch: 355, Train Loss: 0.000034, Validation Loss: 0.000097\n",
            "Epoch: 356, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 357, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 358, Train Loss: 0.000034, Validation Loss: 0.000097\n",
            "Epoch: 359, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 360, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 361, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 362, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 363, Train Loss: 0.000033, Validation Loss: 0.000099\n",
            "Epoch: 364, Train Loss: 0.000034, Validation Loss: 0.000097\n",
            "Epoch: 365, Train Loss: 0.000033, Validation Loss: 0.000100\n",
            "Epoch: 366, Train Loss: 0.000034, Validation Loss: 0.000096\n",
            "Epoch: 367, Train Loss: 0.000034, Validation Loss: 0.000098\n",
            "Epoch: 368, Train Loss: 0.000034, Validation Loss: 0.000097\n",
            "Epoch: 369, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 370, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 371, Train Loss: 0.000033, Validation Loss: 0.000099\n",
            "Epoch: 372, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 373, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 374, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 375, Train Loss: 0.000033, Validation Loss: 0.000096\n",
            "Epoch: 376, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 377, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 378, Train Loss: 0.000033, Validation Loss: 0.000096\n",
            "Epoch: 379, Train Loss: 0.000033, Validation Loss: 0.000096\n",
            "Epoch: 380, Train Loss: 0.000033, Validation Loss: 0.000098\n",
            "Epoch: 381, Train Loss: 0.000034, Validation Loss: 0.000096\n",
            "Epoch: 382, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 383, Train Loss: 0.000032, Validation Loss: 0.000096\n",
            "Epoch: 384, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 385, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 386, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 387, Train Loss: 0.000033, Validation Loss: 0.000097\n",
            "Epoch: 388, Train Loss: 0.000033, Validation Loss: 0.000096\n",
            "Epoch: 389, Train Loss: 0.000033, Validation Loss: 0.000094\n",
            "Epoch: 390, Train Loss: 0.000032, Validation Loss: 0.000096\n",
            "Epoch: 391, Train Loss: 0.000032, Validation Loss: 0.000096\n",
            "Epoch: 392, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 393, Train Loss: 0.000032, Validation Loss: 0.000097\n",
            "Epoch: 394, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 395, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 396, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 397, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 398, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 399, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 400, Train Loss: 0.000032, Validation Loss: 0.000092\n",
            "Epoch: 401, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 402, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 403, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 404, Train Loss: 0.000033, Validation Loss: 0.000093\n",
            "Epoch: 405, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 406, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 407, Train Loss: 0.000033, Validation Loss: 0.000095\n",
            "Epoch: 408, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 409, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 410, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 411, Train Loss: 0.000032, Validation Loss: 0.000091\n",
            "Epoch: 412, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 413, Train Loss: 0.000032, Validation Loss: 0.000092\n",
            "Epoch: 414, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 415, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 416, Train Loss: 0.000032, Validation Loss: 0.000091\n",
            "Epoch: 417, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 418, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 419, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 420, Train Loss: 0.000032, Validation Loss: 0.000094\n",
            "Epoch: 421, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 422, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 423, Train Loss: 0.000031, Validation Loss: 0.000091\n",
            "Epoch: 424, Train Loss: 0.000032, Validation Loss: 0.000092\n",
            "Epoch: 425, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 426, Train Loss: 0.000031, Validation Loss: 0.000092\n",
            "Epoch: 427, Train Loss: 0.000031, Validation Loss: 0.000092\n",
            "Epoch: 428, Train Loss: 0.000032, Validation Loss: 0.000090\n",
            "Epoch: 429, Train Loss: 0.000034, Validation Loss: 0.000092\n",
            "Epoch: 430, Train Loss: 0.000032, Validation Loss: 0.000093\n",
            "Epoch: 431, Train Loss: 0.000031, Validation Loss: 0.000089\n",
            "Epoch: 432, Train Loss: 0.000031, Validation Loss: 0.000091\n",
            "Epoch: 433, Train Loss: 0.000031, Validation Loss: 0.000091\n",
            "Epoch: 434, Train Loss: 0.000031, Validation Loss: 0.000091\n",
            "Epoch: 435, Train Loss: 0.000031, Validation Loss: 0.000089\n",
            "Epoch: 436, Train Loss: 0.000031, Validation Loss: 0.000090\n",
            "Epoch: 437, Train Loss: 0.000031, Validation Loss: 0.000090\n",
            "Epoch: 438, Train Loss: 0.000032, Validation Loss: 0.000095\n",
            "Epoch: 439, Train Loss: 0.000032, Validation Loss: 0.000089\n",
            "Epoch: 440, Train Loss: 0.000032, Validation Loss: 0.000090\n",
            "Epoch: 441, Train Loss: 0.000031, Validation Loss: 0.000093\n",
            "Epoch: 442, Train Loss: 0.000031, Validation Loss: 0.000090\n",
            "Epoch: 443, Train Loss: 0.000031, Validation Loss: 0.000088\n",
            "Epoch: 444, Train Loss: 0.000031, Validation Loss: 0.000090\n",
            "Epoch: 445, Train Loss: 0.000031, Validation Loss: 0.000091\n",
            "Epoch: 446, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 447, Train Loss: 0.000032, Validation Loss: 0.000091\n",
            "Epoch: 448, Train Loss: 0.000032, Validation Loss: 0.000091\n",
            "Epoch: 449, Train Loss: 0.000031, Validation Loss: 0.000090\n",
            "Epoch: 450, Train Loss: 0.000030, Validation Loss: 0.000089\n",
            "Epoch: 451, Train Loss: 0.000030, Validation Loss: 0.000090\n",
            "Epoch: 452, Train Loss: 0.000030, Validation Loss: 0.000089\n",
            "Epoch: 453, Train Loss: 0.000031, Validation Loss: 0.000089\n",
            "Epoch: 454, Train Loss: 0.000030, Validation Loss: 0.000091\n",
            "Epoch: 455, Train Loss: 0.000030, Validation Loss: 0.000087\n",
            "Epoch: 456, Train Loss: 0.000030, Validation Loss: 0.000089\n",
            "Epoch: 457, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 458, Train Loss: 0.000031, Validation Loss: 0.000087\n",
            "Epoch: 459, Train Loss: 0.000030, Validation Loss: 0.000089\n",
            "Epoch: 460, Train Loss: 0.000030, Validation Loss: 0.000090\n",
            "Epoch: 461, Train Loss: 0.000029, Validation Loss: 0.000088\n",
            "Epoch: 462, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 463, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 464, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 465, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 466, Train Loss: 0.000030, Validation Loss: 0.000089\n",
            "Epoch: 467, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 468, Train Loss: 0.000030, Validation Loss: 0.000088\n",
            "Epoch: 469, Train Loss: 0.000031, Validation Loss: 0.000087\n",
            "Epoch: 470, Train Loss: 0.000029, Validation Loss: 0.000088\n",
            "Epoch: 471, Train Loss: 0.000029, Validation Loss: 0.000087\n",
            "Epoch: 472, Train Loss: 0.000030, Validation Loss: 0.000085\n",
            "Epoch: 473, Train Loss: 0.000030, Validation Loss: 0.000087\n",
            "Epoch: 474, Train Loss: 0.000029, Validation Loss: 0.000087\n",
            "Epoch: 475, Train Loss: 0.000030, Validation Loss: 0.000087\n",
            "Epoch: 476, Train Loss: 0.000029, Validation Loss: 0.000087\n",
            "Epoch: 477, Train Loss: 0.000029, Validation Loss: 0.000086\n",
            "Epoch: 478, Train Loss: 0.000029, Validation Loss: 0.000086\n",
            "Epoch: 479, Train Loss: 0.000029, Validation Loss: 0.000086\n",
            "Epoch: 480, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 481, Train Loss: 0.000029, Validation Loss: 0.000088\n",
            "Epoch: 482, Train Loss: 0.000030, Validation Loss: 0.000087\n",
            "Epoch: 483, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 484, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 485, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 486, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 487, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 488, Train Loss: 0.000029, Validation Loss: 0.000084\n",
            "Epoch: 489, Train Loss: 0.000029, Validation Loss: 0.000086\n",
            "Epoch: 490, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 491, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 492, Train Loss: 0.000029, Validation Loss: 0.000084\n",
            "Epoch: 493, Train Loss: 0.000030, Validation Loss: 0.000085\n",
            "Epoch: 494, Train Loss: 0.000030, Validation Loss: 0.000085\n",
            "Epoch: 495, Train Loss: 0.000030, Validation Loss: 0.000087\n",
            "Epoch: 496, Train Loss: 0.000030, Validation Loss: 0.000086\n",
            "Epoch: 497, Train Loss: 0.000030, Validation Loss: 0.000083\n",
            "Epoch: 498, Train Loss: 0.000029, Validation Loss: 0.000083\n",
            "Epoch: 499, Train Loss: 0.000028, Validation Loss: 0.000087\n",
            "Epoch: 500, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 501, Train Loss: 0.000028, Validation Loss: 0.000085\n",
            "Epoch: 502, Train Loss: 0.000028, Validation Loss: 0.000084\n",
            "Epoch: 503, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 504, Train Loss: 0.000029, Validation Loss: 0.000084\n",
            "Epoch: 505, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 506, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 507, Train Loss: 0.000029, Validation Loss: 0.000088\n",
            "Epoch: 508, Train Loss: 0.000029, Validation Loss: 0.000083\n",
            "Epoch: 509, Train Loss: 0.000030, Validation Loss: 0.000085\n",
            "Epoch: 510, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 511, Train Loss: 0.000029, Validation Loss: 0.000085\n",
            "Epoch: 512, Train Loss: 0.000029, Validation Loss: 0.000086\n",
            "Epoch: 513, Train Loss: 0.000029, Validation Loss: 0.000084\n",
            "Epoch: 514, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 515, Train Loss: 0.000029, Validation Loss: 0.000087\n",
            "Epoch: 516, Train Loss: 0.000028, Validation Loss: 0.000081\n",
            "Epoch: 517, Train Loss: 0.000028, Validation Loss: 0.000085\n",
            "Epoch: 518, Train Loss: 0.000028, Validation Loss: 0.000084\n",
            "Epoch: 519, Train Loss: 0.000028, Validation Loss: 0.000087\n",
            "Epoch: 520, Train Loss: 0.000029, Validation Loss: 0.000084\n",
            "Epoch: 521, Train Loss: 0.000029, Validation Loss: 0.000082\n",
            "Epoch: 522, Train Loss: 0.000028, Validation Loss: 0.000081\n",
            "Epoch: 523, Train Loss: 0.000027, Validation Loss: 0.000082\n",
            "Epoch: 524, Train Loss: 0.000029, Validation Loss: 0.000083\n",
            "Epoch: 525, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 526, Train Loss: 0.000027, Validation Loss: 0.000083\n",
            "Epoch: 527, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 528, Train Loss: 0.000029, Validation Loss: 0.000083\n",
            "Epoch: 529, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 530, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 531, Train Loss: 0.000027, Validation Loss: 0.000082\n",
            "Epoch: 532, Train Loss: 0.000027, Validation Loss: 0.000083\n",
            "Epoch: 533, Train Loss: 0.000028, Validation Loss: 0.000084\n",
            "Epoch: 534, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 535, Train Loss: 0.000028, Validation Loss: 0.000079\n",
            "Epoch: 536, Train Loss: 0.000028, Validation Loss: 0.000083\n",
            "Epoch: 537, Train Loss: 0.000028, Validation Loss: 0.000080\n",
            "Epoch: 538, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 539, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 540, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 541, Train Loss: 0.000028, Validation Loss: 0.000080\n",
            "Epoch: 542, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 543, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 544, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 545, Train Loss: 0.000029, Validation Loss: 0.000079\n",
            "Epoch: 546, Train Loss: 0.000027, Validation Loss: 0.000078\n",
            "Epoch: 547, Train Loss: 0.000027, Validation Loss: 0.000079\n",
            "Epoch: 548, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 549, Train Loss: 0.000026, Validation Loss: 0.000078\n",
            "Epoch: 550, Train Loss: 0.000026, Validation Loss: 0.000081\n",
            "Epoch: 551, Train Loss: 0.000028, Validation Loss: 0.000080\n",
            "Epoch: 552, Train Loss: 0.000027, Validation Loss: 0.000085\n",
            "Epoch: 553, Train Loss: 0.000029, Validation Loss: 0.000078\n",
            "Epoch: 554, Train Loss: 0.000027, Validation Loss: 0.000081\n",
            "Epoch: 555, Train Loss: 0.000027, Validation Loss: 0.000078\n",
            "Epoch: 556, Train Loss: 0.000027, Validation Loss: 0.000080\n",
            "Epoch: 557, Train Loss: 0.000027, Validation Loss: 0.000077\n",
            "Epoch: 558, Train Loss: 0.000027, Validation Loss: 0.000077\n",
            "Epoch: 559, Train Loss: 0.000027, Validation Loss: 0.000082\n",
            "Epoch: 560, Train Loss: 0.000026, Validation Loss: 0.000080\n",
            "Epoch: 561, Train Loss: 0.000027, Validation Loss: 0.000077\n",
            "Epoch: 562, Train Loss: 0.000026, Validation Loss: 0.000076\n",
            "Epoch: 563, Train Loss: 0.000027, Validation Loss: 0.000079\n",
            "Epoch: 564, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 565, Train Loss: 0.000026, Validation Loss: 0.000079\n",
            "Epoch: 566, Train Loss: 0.000027, Validation Loss: 0.000078\n",
            "Epoch: 567, Train Loss: 0.000026, Validation Loss: 0.000079\n",
            "Epoch: 568, Train Loss: 0.000025, Validation Loss: 0.000076\n",
            "Epoch: 569, Train Loss: 0.000027, Validation Loss: 0.000077\n",
            "Epoch: 570, Train Loss: 0.000026, Validation Loss: 0.000076\n",
            "Epoch: 571, Train Loss: 0.000026, Validation Loss: 0.000076\n",
            "Epoch: 572, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 573, Train Loss: 0.000026, Validation Loss: 0.000075\n",
            "Epoch: 574, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 575, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 576, Train Loss: 0.000026, Validation Loss: 0.000075\n",
            "Epoch: 577, Train Loss: 0.000026, Validation Loss: 0.000076\n",
            "Epoch: 578, Train Loss: 0.000026, Validation Loss: 0.000076\n",
            "Epoch: 579, Train Loss: 0.000026, Validation Loss: 0.000075\n",
            "Epoch: 580, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 581, Train Loss: 0.000026, Validation Loss: 0.000081\n",
            "Epoch: 582, Train Loss: 0.000028, Validation Loss: 0.000077\n",
            "Epoch: 583, Train Loss: 0.000027, Validation Loss: 0.000083\n",
            "Epoch: 584, Train Loss: 0.000027, Validation Loss: 0.000078\n",
            "Epoch: 585, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 586, Train Loss: 0.000026, Validation Loss: 0.000074\n",
            "Epoch: 587, Train Loss: 0.000025, Validation Loss: 0.000076\n",
            "Epoch: 588, Train Loss: 0.000025, Validation Loss: 0.000075\n",
            "Epoch: 589, Train Loss: 0.000025, Validation Loss: 0.000074\n",
            "Epoch: 590, Train Loss: 0.000025, Validation Loss: 0.000075\n",
            "Epoch: 591, Train Loss: 0.000026, Validation Loss: 0.000074\n",
            "Epoch: 592, Train Loss: 0.000025, Validation Loss: 0.000077\n",
            "Epoch: 593, Train Loss: 0.000024, Validation Loss: 0.000074\n",
            "Epoch: 594, Train Loss: 0.000025, Validation Loss: 0.000076\n",
            "Epoch: 595, Train Loss: 0.000026, Validation Loss: 0.000082\n",
            "Epoch: 596, Train Loss: 0.000026, Validation Loss: 0.000077\n",
            "Epoch: 597, Train Loss: 0.000025, Validation Loss: 0.000074\n",
            "Epoch: 598, Train Loss: 0.000026, Validation Loss: 0.000072\n",
            "Epoch: 599, Train Loss: 0.000026, Validation Loss: 0.000075\n",
            "Epoch: 600, Train Loss: 0.000024, Validation Loss: 0.000073\n",
            "Epoch: 601, Train Loss: 0.000023, Validation Loss: 0.000074\n",
            "Epoch: 602, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 603, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 604, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 605, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 606, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 607, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 608, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 609, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 610, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 611, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 612, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 613, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 614, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 615, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 616, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 617, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 618, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 619, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 620, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 621, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 622, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 623, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 624, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 625, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 626, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 627, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 628, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 629, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 630, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 631, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 632, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 633, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 634, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 635, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 636, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 637, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 638, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 639, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 640, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 641, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 642, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 643, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 644, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 645, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 646, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 647, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 648, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 649, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 650, Train Loss: 0.000023, Validation Loss: 0.000073\n",
            "Epoch: 651, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 652, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 653, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 654, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 655, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 656, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 657, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 658, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 659, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 660, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 661, Train Loss: 0.000022, Validation Loss: 0.000073\n",
            "Epoch: 662, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 663, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 664, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 665, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 666, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 667, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 668, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 669, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 670, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 671, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 672, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 673, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 674, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 675, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 676, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 677, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 678, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 679, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 680, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 681, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 682, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 683, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 684, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 685, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 686, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 687, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 688, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 689, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 690, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 691, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 692, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 693, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 694, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 695, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 696, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 697, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 698, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 699, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 700, Train Loss: 0.000023, Validation Loss: 0.000072\n",
            "Epoch: 701, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 702, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 703, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 704, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 705, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 706, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 707, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 708, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 709, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 710, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 711, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 712, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 713, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 714, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 715, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 716, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 717, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 718, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 719, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 720, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 721, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 722, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 723, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 724, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 725, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 726, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 727, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 728, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 729, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 730, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 731, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 732, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 733, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 734, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 735, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 736, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 737, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 738, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 739, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 740, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 741, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 742, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 743, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 744, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 745, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 746, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 747, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 748, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 749, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 750, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 751, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 752, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 753, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 754, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 755, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 756, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 757, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 758, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 759, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 760, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 761, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 762, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 763, Train Loss: 0.000022, Validation Loss: 0.000072\n",
            "Epoch: 764, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 765, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 766, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 767, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 768, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 769, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 770, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 771, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 772, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 773, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 774, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 775, Train Loss: 0.000022, Validation Loss: 0.000070\n",
            "Epoch: 776, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 777, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 778, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 779, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 780, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 781, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 782, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 783, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 784, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 785, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 786, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 787, Train Loss: 0.000022, Validation Loss: 0.000070\n",
            "Epoch: 788, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 789, Train Loss: 0.000022, Validation Loss: 0.000070\n",
            "Epoch: 790, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 791, Train Loss: 0.000022, Validation Loss: 0.000070\n",
            "Epoch: 792, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 793, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 794, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 795, Train Loss: 0.000022, Validation Loss: 0.000070\n",
            "Epoch: 796, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 797, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 798, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Epoch: 799, Train Loss: 0.000022, Validation Loss: 0.000071\n",
            "Training completed. Model saved in checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250411_170218\n",
            "Best validation loss: 0.000070 at epoch 795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from visualization import (\n",
        "    plot_training_history,\n",
        "    plot_resolution_comparison,\n",
        "    plot_l2_error_by_resolution,\n",
        "    plot_error_distributions,\n",
        "    plot_ibvp_sol_heatmap,\n",
        "    plot_model_errors_temporal\n",
        ")\n",
        "from utils import (\n",
        "    print_bold,\n",
        "    evaluate_direct,\n",
        "    evaluate_autoregressive\n",
        ")\n",
        "\n",
        "res_dir = Path('results')\n",
        "res_dir.mkdir(exist_ok=True)\n",
        "time_res_dir = res_dir / 'time'\n",
        "time_res_dir.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "hsxzgUzQzcP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_model(checkpoint_dir: str) -> torch.nn.Module:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    checkpoint_dir = Path(checkpoint_dir)\n",
        "\n",
        "    with open(checkpoint_dir / 'training_config.json', 'r') as f:\n",
        "        config_dict = json.load(f)\n",
        "\n",
        "    model_config = config_dict['model_config']\n",
        "    model_args = {k: v for k, v in model_config.items()}\n",
        "    model = GatedFNO1d(**model_args)\n",
        "\n",
        "    model.load_state_dict(torch.load(checkpoint_dir / 'model.pth', weights_only=True))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "6SF9RbP7Ild4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time-independent evaluation\n",
        "data_mode = \"onetoone\"\n",
        "fno_folders = sorted(Path(f'checkpoints/{data_mode}').glob('fno_*'),\n",
        "                    key=lambda d: d.stat().st_mtime)"
      ],
      "metadata": {
        "id": "W7td2nqMzcNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fno_folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hqp0ua80UG3",
        "outputId": "df810a4b-5190-499e-aa86-ec3815ed6a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250102_145624'),\n",
              " PosixPath('checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250411_183037')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(fno_folders[-1])\n",
        "print(f\"Loading Custom FNO from: {fno_folders[-1]}\")\n",
        "\n",
        "print(\"Plotting training history...\")\n",
        "plot_training_history(fno_folders[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAo1Xa6_IRML",
        "outputId": "2816340c-edd7-4ab7-c632-ff74a390f1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Custom FNO from: checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250411_183037\n",
            "Plotting training history...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(fno_folders[-1])\n",
        "print(f\"Loading Custom FNO from: {fno_folders[-1]}\")\n",
        "\n",
        "print(\"Plotting training history...\")\n",
        "plot_training_history(fno_folders[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96UIgbKPzcKt",
        "outputId": "adccc92c-1760-487a-ed9e-4478990636e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Custom FNO from: checkpoints/onetoone/fno_m30_w16_d2_lr0.001_20250411_170218\n",
            "Plotting training history...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 1: Evaluating FNO model from One-to-One training on standard test set...\")\n",
        "result, test_data = evaluate_direct(model, \"data/test_sol.npy\")\n",
        "print(f\"Resolution: {test_data[0].shape[0]}\")\n",
        "print(f\"Average Relative L2 Error Over {test_data[0].shape[0]} Testing Trajectories (resolution {test_data[0].shape[1]}):\")\n",
        "print(f\"Custom FNO: {result['error']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpgCj-6uJAiK",
        "outputId": "8c080e3a-bb48-43be-c21f-6e12576aceeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 1: Evaluating FNO model from One-to-One training on standard test set...\u001b[0m\n",
            "Resolution: 128\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 9.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 1: Evaluating FNO model from One-to-One training on standard test set...\")\n",
        "result, test_data = evaluate_direct(model, \"data/test_sol.npy\")\n",
        "print(f\"Resolution: {test_data[0].shape[0]}\")\n",
        "print(f\"Average Relative L2 Error Over {test_data[0].shape[0]} Testing Trajectories (resolution {test_data[0].shape[1]}):\")\n",
        "print(f\"Custom FNO: {result['error']:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMPI2K0c0hJ1",
        "outputId": "25ae5408-2841-4b16-de8c-b2403a458a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 1: Evaluating FNO model from One-to-One training on standard test set...\u001b[0m\n",
            "Resolution: 128\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 5.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 2: Testing on different resolutions:\")\n",
        "resolutions = [32, 64, 96, 128]\n",
        "resolution_results = {'Custom FNO': {'errors': [], 'predictions': {}, 'abs_errors': []}}\n",
        "\n",
        "for res in resolutions:\n",
        "    print(f\"Resolution: {res}\")\n",
        "    result, test_data = evaluate_direct(model, f\"data/test_sol_res_{res}.npy\", end_idx=1)\n",
        "\n",
        "    print(f\"Average Relative L2 Error Over {test_data[0].shape[0]} Testing Trajectories (resolution {test_data[0].shape[1]}):\")\n",
        "    print(f\"Custom FNO: {result['error']:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "    resolution_results['Custom FNO']['errors'].append(result['error'])\n",
        "    resolution_results['Custom FNO']['predictions'][res] = result['predictions']\n",
        "\n",
        "resolution_data = {}\n",
        "for res in resolutions:\n",
        "    dataset = OneToOne(\n",
        "        which=\"testing\",\n",
        "        data_path=f\"data/test_sol_res_{res}.npy\",\n",
        "        start_idx=0,\n",
        "        end_idx=1\n",
        "    )\n",
        "\n",
        "    input_data = torch.stack((\n",
        "        dataset.u_start,\n",
        "        dataset.v_start,\n",
        "        dataset.x_grid.repeat(len(dataset.u_start), 1),\n",
        "        torch.full_like(dataset.u_start, dataset.dt)\n",
        "    ), dim=-1)\n",
        "\n",
        "    resolution_data[res] = {'custom': (input_data, dataset.u_end)}\n",
        "\n",
        "plot_resolution_comparison(model, resolution_data, resolution_results, save_dir=res_dir)\n",
        "plot_l2_error_by_resolution(resolution_results, resolutions, save_dir=res_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I6_KY0IJEo4",
        "outputId": "9f4c718d-37c9-47dd-8b74-62f7df26f65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 2: Testing on different resolutions:\u001b[0m\n",
            "Resolution: 32\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 22.66%\n",
            "--------------------------------------------------\n",
            "Resolution: 64\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 8.62%\n",
            "--------------------------------------------------\n",
            "Resolution: 96\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 9.47%\n",
            "--------------------------------------------------\n",
            "Resolution: 128\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 9.74%\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 2: Testing on different resolutions:\")\n",
        "resolutions = [32, 64, 96, 128]\n",
        "resolution_results = {'Custom FNO': {'errors': [], 'predictions': {}, 'abs_errors': []}}\n",
        "\n",
        "for res in resolutions:\n",
        "    print(f\"Resolution: {res}\")\n",
        "    result, test_data = evaluate_direct(model, f\"data/test_sol_res_{res}.npy\", end_idx=1)\n",
        "\n",
        "    print(f\"Average Relative L2 Error Over {test_data[0].shape[0]} Testing Trajectories (resolution {test_data[0].shape[1]}):\")\n",
        "    print(f\"Custom FNO: {result['error']:.2f}%\")\n",
        "    print(\"-\" * 50)\n",
        "    resolution_results['Custom FNO']['errors'].append(result['error'])\n",
        "    resolution_results['Custom FNO']['predictions'][res] = result['predictions']\n",
        "\n",
        "resolution_data = {}\n",
        "for res in resolutions:\n",
        "    dataset = OneToOne(\n",
        "        which=\"testing\",\n",
        "        data_path=f\"data/test_sol_res_{res}.npy\",\n",
        "        start_idx=0,\n",
        "        end_idx=1\n",
        "    )\n",
        "\n",
        "    input_data = torch.stack((\n",
        "        dataset.u_start,\n",
        "        dataset.v_start,\n",
        "        dataset.x_grid.repeat(len(dataset.u_start), 1),\n",
        "        torch.full_like(dataset.u_start, dataset.dt)\n",
        "    ), dim=-1)\n",
        "\n",
        "    resolution_data[res] = {'custom': (input_data, dataset.u_end)}\n",
        "\n",
        "plot_resolution_comparison(model, resolution_data, resolution_results, save_dir=res_dir)\n",
        "plot_l2_error_by_resolution(resolution_results, resolutions, save_dir=res_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DjzyTnl0scJ",
        "outputId": "689e30cc-e284-4e58-b8cd-332a781d2e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 2: Testing on different resolutions:\u001b[0m\n",
            "Resolution: 32\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 20.07%\n",
            "--------------------------------------------------\n",
            "Resolution: 64\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 4.99%\n",
            "--------------------------------------------------\n",
            "Resolution: 96\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 5.92%\n",
            "--------------------------------------------------\n",
            "Resolution: 128\n",
            "Average Relative L2 Error Over 128 Testing Trajectories (resolution 4):\n",
            "Custom FNO: 6.39%\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 3: Testing on OOD dataset:\")\n",
        "\n",
        "in_result, in_data = evaluate_direct(model, \"data/test_sol.npy\")\n",
        "ood_result, ood_data = evaluate_direct(model, \"data/test_sol_OOD.npy\", end_idx=1)\n",
        "\n",
        "print(f\"In-Distribution - Average Relative L2 Error Over {in_data[0].shape[0]} Testing Trajectories:\")\n",
        "print(f\"Custom FNO: {in_result['error']:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"Out-of-Distribution - Average Relative L2 Error Over {ood_data[0].shape[0]} Testing Trajectories:\")\n",
        "print(f\"Custom FNO: {ood_result['error']:.2f}%\")\n",
        "\n",
        "in_dist_results = {'Custom FNO': in_result}\n",
        "ood_results = {'Custom FNO': ood_result}\n",
        "plot_error_distributions(in_dist_results, ood_results, save_path=res_dir / 'error_distributions.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V5zXunLKyNY",
        "outputId": "167dba59-39da-4d49-d9fd-2436bd46442f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 3: Testing on OOD dataset:\u001b[0m\n",
            "In-Distribution - Average Relative L2 Error Over 128 Testing Trajectories:\n",
            "Custom FNO: 9.18%\n",
            "--------------------------------------------------\n",
            "Out-of-Distribution - Average Relative L2 Error Over 128 Testing Trajectories:\n",
            "Custom FNO: 13.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_bold(\"Task 3: Testing on OOD dataset:\")\n",
        "\n",
        "in_result, in_data = evaluate_direct(model, \"data/test_sol.npy\")\n",
        "ood_result, ood_data = evaluate_direct(model, \"data/test_sol_OOD.npy\", end_idx=1)\n",
        "\n",
        "print(f\"In-Distribution - Average Relative L2 Error Over {in_data[0].shape[0]} Testing Trajectories:\")\n",
        "print(f\"Custom FNO: {in_result['error']:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"Out-of-Distribution - Average Relative L2 Error Over {ood_data[0].shape[0]} Testing Trajectories:\")\n",
        "print(f\"Custom FNO: {ood_result['error']:.2f}%\")\n",
        "\n",
        "in_dist_results = {'Custom FNO': in_result}\n",
        "ood_results = {'Custom FNO': ood_result}\n",
        "plot_error_distributions(in_dist_results, ood_results, save_path=res_dir / 'error_distributions.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhA76k41Cqr",
        "outputId": "86a9cc5b-4737-4dcf-fee2-b2c61623cffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mTask 3: Testing on OOD dataset:\u001b[0m\n",
            "In-Distribution - Average Relative L2 Error Over 128 Testing Trajectories:\n",
            "Custom FNO: 5.21%\n",
            "--------------------------------------------------\n",
            "Out-of-Distribution - Average Relative L2 Error Over 128 Testing Trajectories:\n",
            "Custom FNO: 9.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "task1_results = task1_evaluation(model, res_dir)\n",
        "task2_results = task2_evaluation(model, res_dir)\n",
        "task3_results = task3_evaluation(model, res_dir)"
      ],
      "metadata": {
        "id": "m-MCRRe7zcIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M885reh5zcFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j8ALXdhdzcDb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}