{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdwmm9U8UQhL"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from timeit import default_timer\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNO_LReLu(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_size,\n",
        "                 out_size\n",
        "                 ):\n",
        "        super(CNO_LReLu, self).__init__()\n",
        "\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "        self.act = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.interpolate(x.unsqueeze(2), size = (1,2 * self.in_size), mode = \"bicubic\", antialias = True)\n",
        "      x = self.act(x)\n",
        "      x = F.interpolate(x, size = (1,self.out_size), mode = \"bicubic\", antialias = True)\n",
        "\n",
        "      return x[:,:,0]\n",
        "\n",
        "class CNOBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 in_size,\n",
        "                 out_size,\n",
        "                 use_bn = True\n",
        "                 ):\n",
        "        super(CNOBlock, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.in_size  = in_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        # We apply Conv -> BN (optional) -> Activation\n",
        "        # Up/Downsampling happens inside Activation\n",
        "        self.convolution = torch.nn.Conv1d(in_channels = self.in_channels,\n",
        "                                           out_channels= self.out_channels,\n",
        "                                           kernel_size = 3,\n",
        "                                           padding     = 1)\n",
        "\n",
        "        if use_bn:\n",
        "          self.batch_norm  = nn.BatchNorm1d(self.out_channels)\n",
        "        else:\n",
        "          self.batch_norm  = nn.Identity()\n",
        "        self.act           = CNO_LReLu(in_size  = self.in_size,\n",
        "                                       out_size = self.out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolution(x)\n",
        "        x = self.batch_norm(x)\n",
        "        return self.act(x)\n",
        "\n",
        "class LiftProjectBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                size,\n",
        "                latent_dim = 64\n",
        "                ):\n",
        "        super(LiftProjectBlock, self).__init__()\n",
        "\n",
        "        self.inter_CNOBlock = CNOBlock(in_channels       = in_channels,\n",
        "                                        out_channels     = latent_dim,\n",
        "                                        in_size          = size,\n",
        "                                        out_size         = size,\n",
        "                                        use_bn           = False)\n",
        "\n",
        "        self.convolution = torch.nn.Conv1d(in_channels  = latent_dim,\n",
        "                                           out_channels = out_channels,\n",
        "                                           kernel_size  = 3,\n",
        "                                           padding      = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.inter_CNOBlock(x)\n",
        "        x = self.convolution(x)\n",
        "        return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 size,\n",
        "                 use_bn = True\n",
        "                 ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.size     = size\n",
        "\n",
        "        # We apply Conv -> BN (optional) -> Activation -> Conv -> BN (optional) -> Skip Connection\n",
        "        # Up/Downsampling happens inside Activation\n",
        "        self.convolution1 = torch.nn.Conv1d(in_channels = self.channels,\n",
        "                                            out_channels= self.channels,\n",
        "                                            kernel_size = 3,\n",
        "                                            padding     = 1)\n",
        "        self.convolution2 = torch.nn.Conv1d(in_channels = self.channels,\n",
        "                                            out_channels= self.channels,\n",
        "                                            kernel_size = 3,\n",
        "                                            padding     = 1)\n",
        "\n",
        "        if use_bn:\n",
        "          self.batch_norm1  = nn.BatchNorm1d(self.channels)\n",
        "          self.batch_norm2  = nn.BatchNorm1d(self.channels)\n",
        "        else:\n",
        "          self.batch_norm1  = nn.Identity()\n",
        "          self.batch_norm2  = nn.Identity()\n",
        "\n",
        "        self.act           = CNO_LReLu(in_size  = self.size,\n",
        "                                       out_size = self.size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.convolution1(x)\n",
        "        out = self.batch_norm1(out)\n",
        "        out = self.act(out)\n",
        "        out = self.convolution2(out)\n",
        "        out = self.batch_norm2(out)\n",
        "        return x + out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 size,\n",
        "                 num_blocks,\n",
        "                 use_bn = True\n",
        "                 ):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.channels = channels\n",
        "        self.size = size\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        self.res_nets = []\n",
        "        for _ in range(self.num_blocks):\n",
        "            self.res_nets.append(ResidualBlock(channels = channels,\n",
        "                                              size = size,\n",
        "                                              use_bn = use_bn))\n",
        "\n",
        "        self.res_nets = torch.nn.Sequential(*self.res_nets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.res_nets[i](x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2OKSfpwpUSEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNO1d(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_channels,           # Number of input/output channels\n",
        "                 width=64,               # Base width for the network\n",
        "                 initial_step=10,        # Number of initial timesteps\n",
        "                 size=None,              # Spatial size of the domain\n",
        "                 N_layers=4,             # Number of (D) or (U) blocks in the network\n",
        "                 N_res=4,                # Number of (R) blocks per level (except the neck)\n",
        "                 N_res_neck=4,           # Number of (R) blocks in the neck\n",
        "                 channel_multiplier=16,  # How the number of channels evolve\n",
        "                 use_bn=True             # Add BN? We do not add BN in lifting/projection layer\n",
        "                ):\n",
        "        super(CNO1d, self).__init__()\n",
        "\n",
        "        # Store parameters\n",
        "        self.num_channels = num_channels\n",
        "        self.initial_step = initial_step\n",
        "        self.width = width\n",
        "\n",
        "        # Determine spatial size if not provided\n",
        "        if size is None:\n",
        "            size = 128  # Default spatial size\n",
        "\n",
        "        # Create the CNO1d architecture\n",
        "        self.model = CNO1d_Core(\n",
        "            in_dim=initial_step * num_channels + 1,  # Input: initial_step channels + grid\n",
        "            out_dim=num_channels,                    # Output: predict next channel values\n",
        "            size=size,\n",
        "            N_layers=N_layers,\n",
        "            N_res=N_res,\n",
        "            N_res_neck=N_res_neck,\n",
        "            channel_multiplier=channel_multiplier,\n",
        "            use_bn=use_bn\n",
        "        )\n",
        "\n",
        "    def forward(self, x, grid):\n",
        "        # x dim = [b, x1, t*v] - same input format as FNO1d\n",
        "        # Combine x and grid to match CNO1d_Core input format\n",
        "        inputs = torch.cat((x, grid), dim=-1)  # concatenate along feature dimension\n",
        "\n",
        "        # Reshape for CNO: [batch, channels, spatial_dim]\n",
        "        batch_size = inputs.shape[0]\n",
        "        spatial_dim = inputs.shape[1]\n",
        "        inputs = inputs.permute(0, 2, 1)  # [batch, features, spatial]\n",
        "\n",
        "        # Forward pass through CNO\n",
        "        out = self.model(inputs)\n",
        "\n",
        "        # Reshape back to expected output format: [batch, spatial, channels, 1]\n",
        "        out = out.permute(0, 2, 1)  # [batch, spatial, channels]\n",
        "        out = out.unsqueeze(-2)     # [batch, spatial, 1, channels]\n",
        "\n",
        "        return out\n",
        "\n",
        "# Core CNO1d implementation (original architecture)\n",
        "class CNO1d_Core(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,                    # Number of input channels\n",
        "                 out_dim,                   # Number of output channels\n",
        "                 size,                      # Input and Output spatial size\n",
        "                 N_layers,                  # Number of (D) or (U) blocks in the network\n",
        "                 N_res = 4,                 # Number of (R) blocks per level (except the neck)\n",
        "                 N_res_neck = 4,            # Number of (R) blocks in the neck\n",
        "                 channel_multiplier = 16,   # How the number of channels evolve\n",
        "                 use_bn = True              # Add BN? We do not add BN in lifting/projection layer\n",
        "                ):\n",
        "        super(CNO1d_Core, self).__init__()\n",
        "\n",
        "        self.N_layers = int(N_layers)         # Number of (D) & (U) Blocks\n",
        "        self.lift_dim = channel_multiplier//2 # Input is lifted to the half of channel_multiplier dimension\n",
        "        self.in_dim   = in_dim\n",
        "        self.out_dim  = out_dim\n",
        "        self.channel_multiplier = channel_multiplier  # The growth of the channels\n",
        "\n",
        "        ######## Num of channels/features - evolution ########\n",
        "        self.encoder_features = [self.lift_dim] # How the features in Encoder evolve (number of features)\n",
        "        for i in range(self.N_layers):\n",
        "            self.encoder_features.append(2 ** i * self.channel_multiplier)\n",
        "\n",
        "        self.decoder_features_in = self.encoder_features[1:] # How the features in Decoder evolve (number of features)\n",
        "        self.decoder_features_in.reverse()\n",
        "        self.decoder_features_out = self.encoder_features[:-1]\n",
        "        self.decoder_features_out.reverse()\n",
        "\n",
        "        for i in range(1, self.N_layers):\n",
        "            self.decoder_features_in[i] = 2*self.decoder_features_in[i] #Pad the outputs of the resnets (we must multiply by 2 then)\n",
        "\n",
        "        ######## Spatial sizes of channels - evolution ########\n",
        "        self.encoder_sizes = []\n",
        "        self.decoder_sizes = []\n",
        "        for i in range(self.N_layers + 1):\n",
        "            self.encoder_sizes.append(size // 2 ** i)\n",
        "            self.decoder_sizes.append(size // 2 ** (self.N_layers - i))\n",
        "\n",
        "        ######## Define Lift and Project blocks ########\n",
        "        self.lift = LiftProjectBlock(in_channels = in_dim,\n",
        "                                    out_channels = self.encoder_features[0],\n",
        "                                    size = size)\n",
        "\n",
        "        self.project = LiftProjectBlock(in_channels = self.encoder_features[0] + self.decoder_features_out[-1],\n",
        "                                        out_channels = out_dim,\n",
        "                                        size = size)\n",
        "\n",
        "        ######## Define Encoder, ED Linker and Decoder networks ########\n",
        "        self.encoder = nn.ModuleList([(CNOBlock(in_channels  = self.encoder_features[i],\n",
        "                                                out_channels = self.encoder_features[i+1],\n",
        "                                                in_size      = self.encoder_sizes[i],\n",
        "                                                out_size     = self.encoder_sizes[i+1],\n",
        "                                                use_bn       = use_bn))\n",
        "                                     for i in range(self.N_layers)])\n",
        "\n",
        "        # After the ResNets are executed, the sizes of encoder and decoder might not match (if out_size>1)\n",
        "        # We must ensure that the sizes are the same, by applying CNO Blocks\n",
        "        self.ED_expansion = nn.ModuleList([(CNOBlock(in_channels = self.encoder_features[i],\n",
        "                                                    out_channels = self.encoder_features[i],\n",
        "                                                    in_size      = self.encoder_sizes[i],\n",
        "                                                    out_size     = self.decoder_sizes[self.N_layers - i],\n",
        "                                                    use_bn       = use_bn))\n",
        "                                         for i in range(self.N_layers + 1)])\n",
        "\n",
        "        self.decoder = nn.ModuleList([(CNOBlock(in_channels  = self.decoder_features_in[i],\n",
        "                                                out_channels = self.decoder_features_out[i],\n",
        "                                                in_size      = self.decoder_sizes[i],\n",
        "                                                out_size     = self.decoder_sizes[i+1],\n",
        "                                                use_bn       = use_bn))\n",
        "                                     for i in range(self.N_layers)])\n",
        "\n",
        "        ####################### Define ResNets Blocks ################################################################\n",
        "        # Here, we define ResNet Blocks.\n",
        "        # Operator UNet:\n",
        "        # Outputs of the middle networks are patched (or padded) to corresponding sets of feature maps in the decoder\n",
        "\n",
        "        self.res_nets = []\n",
        "        self.N_res = int(N_res)\n",
        "        self.N_res_neck = int(N_res_neck)\n",
        "\n",
        "        # Define the ResNet networks (before the neck)\n",
        "        for l in range(self.N_layers):\n",
        "              self.res_nets.append(ResNet(channels = self.encoder_features[l],\n",
        "                                        size = self.encoder_sizes[l],\n",
        "                                        num_blocks = self.N_res,\n",
        "                                        use_bn = use_bn))\n",
        "\n",
        "        self.res_net_neck = ResNet(channels = self.encoder_features[self.N_layers],\n",
        "                                  size = self.encoder_sizes[self.N_layers],\n",
        "                                  num_blocks = self.N_res_neck,\n",
        "                                  use_bn = use_bn)\n",
        "\n",
        "        self.res_nets = torch.nn.ModuleList(self.res_nets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lift(x) #Execute Lift\n",
        "        skip = []\n",
        "\n",
        "        # Execute Encoder\n",
        "        for i in range(self.N_layers):\n",
        "            #Apply ResNet & save the result\n",
        "            y = self.res_nets[i](x)\n",
        "            skip.append(y)\n",
        "\n",
        "            # Apply (D) block\n",
        "            x = self.encoder[i](x)\n",
        "\n",
        "        # Apply the deepest ResNet (bottle neck)\n",
        "        x = self.res_net_neck(x)\n",
        "\n",
        "        # Execute Decoder\n",
        "        for i in range(self.N_layers):\n",
        "            # Apply (I) block (ED_expansion) & cat if needed\n",
        "            if i == 0:\n",
        "                x = self.ED_expansion[self.N_layers - i](x) #BottleNeck : no cat\n",
        "            else:\n",
        "                x = torch.cat((x, self.ED_expansion[self.N_layers - i](skip[-i])),1)\n",
        "\n",
        "            # Apply (U) block\n",
        "            x = self.decoder[i](x)\n",
        "\n",
        "        # Cat & Execute Projection\n",
        "        x = torch.cat((x, self.ED_expansion[0](skip[0])),1)\n",
        "        x = self.project(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "rqgpkOw4USCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FNODatasetSingle(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        filename,\n",
        "        initial_step=10,\n",
        "        saved_folder=\"../data/\",\n",
        "        reduced_resolution=1,\n",
        "        reduced_resolution_t=1,\n",
        "        reduced_batch=1,\n",
        "        if_test=False,\n",
        "        test_ratio=0.1,\n",
        "        num_samples_max=-1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        :param filename: filename that contains the dataset\n",
        "        :type filename: STR\n",
        "        :param filenum: array containing indices of filename included in the dataset\n",
        "        :type filenum: ARRAY\n",
        "        :param initial_step: time steps taken as initial condition, defaults to 10\n",
        "        :type initial_step: INT, optional\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Define path to files\n",
        "        root_path = Path(Path(saved_folder).resolve()) / filename\n",
        "        if filename[-2:] != \"h5\":\n",
        "            # print(\".HDF5 file extension is assumed hereafter\")\n",
        "\n",
        "            with h5py.File(root_path, \"r\") as f:\n",
        "                keys = list(f.keys())\n",
        "                keys.sort()\n",
        "                if \"tensor\" not in keys:\n",
        "                    _data = np.array(\n",
        "                        f[\"density\"], dtype=np.float32\n",
        "                    )  # batch, time, x,...\n",
        "                    idx_cfd = _data.shape\n",
        "                    if len(idx_cfd) == 3:  # 1D\n",
        "                        self.data = np.zeros(\n",
        "                            [\n",
        "                                idx_cfd[0] // reduced_batch,\n",
        "                                idx_cfd[2] // reduced_resolution,\n",
        "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
        "                                3,\n",
        "                            ],\n",
        "                            dtype=np.float32,\n",
        "                        )\n",
        "                        # density\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
        "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
        "                        # pressure\n",
        "                        _data = np.array(\n",
        "                            f[\"pressure\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
        "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
        "                        # Vx\n",
        "                        _data = np.array(\n",
        "                            f[\"Vx\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
        "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
        "\n",
        "                        self.grid = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
        "                        self.grid = torch.tensor(\n",
        "                            self.grid[::reduced_resolution], dtype=torch.float\n",
        "                        ).unsqueeze(-1)\n",
        "                        # print(self.data.shape)\n",
        "                    if len(idx_cfd) == 4:  # 2D\n",
        "                        self.data = np.zeros(\n",
        "                            [\n",
        "                                idx_cfd[0] // reduced_batch,\n",
        "                                idx_cfd[2] // reduced_resolution,\n",
        "                                idx_cfd[3] // reduced_resolution,\n",
        "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
        "                                4,\n",
        "                            ],\n",
        "                            dtype=np.float32,\n",
        "                        )\n",
        "                        # density\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
        "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
        "                        # pressure\n",
        "                        _data = np.array(\n",
        "                            f[\"pressure\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
        "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
        "                        # Vx\n",
        "                        _data = np.array(\n",
        "                            f[\"Vx\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
        "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
        "                        # Vy\n",
        "                        _data = np.array(\n",
        "                            f[\"Vy\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 1))\n",
        "                        self.data[..., 3] = _data  # batch, x, t, ch\n",
        "\n",
        "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
        "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
        "                        x = torch.tensor(x, dtype=torch.float)\n",
        "                        y = torch.tensor(y, dtype=torch.float)\n",
        "                        X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
        "                        self.grid = torch.stack((X, Y), axis=-1)[\n",
        "                            ::reduced_resolution, ::reduced_resolution\n",
        "                        ]\n",
        "\n",
        "                    if len(idx_cfd) == 5:  # 3D\n",
        "                        self.data = np.zeros(\n",
        "                            [\n",
        "                                idx_cfd[0] // reduced_batch,\n",
        "                                idx_cfd[2] // reduced_resolution,\n",
        "                                idx_cfd[3] // reduced_resolution,\n",
        "                                idx_cfd[4] // reduced_resolution,\n",
        "                                mt.ceil(idx_cfd[1] / reduced_resolution_t),\n",
        "                                5,\n",
        "                            ],\n",
        "                            dtype=np.float32,\n",
        "                        )\n",
        "                        # density\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
        "                        self.data[..., 0] = _data  # batch, x, t, ch\n",
        "                        # pressure\n",
        "                        _data = np.array(\n",
        "                            f[\"pressure\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
        "                        self.data[..., 1] = _data  # batch, x, t, ch\n",
        "                        # Vx\n",
        "                        _data = np.array(\n",
        "                            f[\"Vx\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
        "                        self.data[..., 2] = _data  # batch, x, t, ch\n",
        "                        # Vy\n",
        "                        _data = np.array(\n",
        "                            f[\"Vy\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
        "                        self.data[..., 3] = _data  # batch, x, t, ch\n",
        "                        # Vz\n",
        "                        _data = np.array(\n",
        "                            f[\"Vz\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data, (0, 2, 3, 4, 1))\n",
        "                        self.data[..., 4] = _data  # batch, x, t, ch\n",
        "\n",
        "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
        "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
        "                        z = np.array(f[\"z-coordinate\"], dtype=np.float32)\n",
        "                        x = torch.tensor(x, dtype=torch.float)\n",
        "                        y = torch.tensor(y, dtype=torch.float)\n",
        "                        z = torch.tensor(z, dtype=torch.float)\n",
        "                        X, Y, Z = torch.meshgrid(x, y, z, indexing=\"ij\")\n",
        "                        self.grid = torch.stack((X, Y, Z), axis=-1)[\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "\n",
        "                else:  # scalar equations\n",
        "                    ## data dim = [t, x1, ..., xd, v]\n",
        "                    _data = np.array(\n",
        "                        f[\"tensor\"], dtype=np.float32\n",
        "                    )  # batch, time, x,...\n",
        "                    if len(_data.shape) == 3:  # 1D\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            ::reduced_resolution_t,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :], (0, 2, 1))\n",
        "                        self.data = _data[:, :, :, None]  # batch, x, t, ch\n",
        "\n",
        "                        self.grid = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
        "                        self.grid = torch.tensor(\n",
        "                            self.grid[::reduced_resolution], dtype=torch.float\n",
        "                        ).unsqueeze(-1)\n",
        "                    if len(_data.shape) == 4:  # 2D Darcy flow\n",
        "                        # u: label\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            :,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :, :], (0, 2, 3, 1))\n",
        "                        # if _data.shape[-1]==1:  # if nt==1\n",
        "                        #    _data = np.tile(_data, (1, 1, 1, 2))\n",
        "                        self.data = _data\n",
        "                        # nu: input\n",
        "                        _data = np.array(\n",
        "                            f[\"nu\"], dtype=np.float32\n",
        "                        )  # batch, time, x,...\n",
        "                        _data = _data[\n",
        "                            ::reduced_batch,\n",
        "                            None,\n",
        "                            ::reduced_resolution,\n",
        "                            ::reduced_resolution,\n",
        "                        ]\n",
        "                        ## convert to [x1, ..., xd, t, v]\n",
        "                        _data = np.transpose(_data[:, :, :, :], (0, 2, 3, 1))\n",
        "                        self.data = np.concatenate([_data, self.data], axis=-1)\n",
        "                        self.data = self.data[:, :, :, :, None]  # batch, x, y, t, ch\n",
        "\n",
        "                        x = np.array(f[\"x-coordinate\"], dtype=np.float32)\n",
        "                        y = np.array(f[\"y-coordinate\"], dtype=np.float32)\n",
        "                        x = torch.tensor(x, dtype=torch.float)\n",
        "                        y = torch.tensor(y, dtype=torch.float)\n",
        "                        X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
        "                        self.grid = torch.stack((X, Y), axis=-1)[\n",
        "                            ::reduced_resolution, ::reduced_resolution\n",
        "                        ]\n",
        "\n",
        "        elif filename[-2:] == \"h5\":  # SWE-2D (RDB)\n",
        "            # print(\".H5 file extension is assumed hereafter\")\n",
        "\n",
        "            with h5py.File(root_path, \"r\") as f:\n",
        "                keys = list(f.keys())\n",
        "                keys.sort()\n",
        "\n",
        "                data_arrays = [\n",
        "                    np.array(f[key][\"data\"], dtype=np.float32) for key in keys\n",
        "                ]\n",
        "                _data = torch.from_numpy(\n",
        "                    np.stack(data_arrays, axis=0)\n",
        "                )  # [batch, nt, nx, ny, nc]\n",
        "                _data = _data[\n",
        "                    ::reduced_batch,\n",
        "                    ::reduced_resolution_t,\n",
        "                    ::reduced_resolution,\n",
        "                    ::reduced_resolution,\n",
        "                    ...,\n",
        "                ]\n",
        "                _data = torch.permute(_data, (0, 2, 3, 1, 4))  # [batch, nx, ny, nt, nc]\n",
        "                gridx, gridy = (\n",
        "                    np.array(f[\"0023\"][\"grid\"][\"x\"], dtype=np.float32),\n",
        "                    np.array(f[\"0023\"][\"grid\"][\"y\"], dtype=np.float32),\n",
        "                )\n",
        "                mgridX, mgridY = np.meshgrid(gridx, gridy, indexing=\"ij\")\n",
        "                _grid = torch.stack(\n",
        "                    (torch.from_numpy(mgridX), torch.from_numpy(mgridY)), axis=-1\n",
        "                )\n",
        "                _grid = _grid[::reduced_resolution, ::reduced_resolution, ...]\n",
        "                _tsteps_t = torch.from_numpy(\n",
        "                    np.array(f[\"0023\"][\"grid\"][\"t\"], dtype=np.float32)\n",
        "                )\n",
        "\n",
        "                tsteps_t = _tsteps_t[::reduced_resolution_t]\n",
        "                self.data = _data\n",
        "                self.grid = _grid\n",
        "                self.tsteps_t = tsteps_t\n",
        "\n",
        "        if num_samples_max > 0:\n",
        "            num_samples_max = min(num_samples_max, self.data.shape[0])\n",
        "        else:\n",
        "            num_samples_max = self.data.shape[0]\n",
        "\n",
        "        test_idx = int(num_samples_max * test_ratio)\n",
        "        if if_test:\n",
        "            self.data = self.data[:test_idx]\n",
        "        else:\n",
        "            self.data = self.data[test_idx:num_samples_max]\n",
        "\n",
        "        # Time steps used as initial conditions\n",
        "        self.initial_step = initial_step\n",
        "\n",
        "        self.data = self.data if torch.is_tensor(self.data) else torch.tensor(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx, ..., : self.initial_step, :], self.data[idx], self.grid"
      ],
      "metadata": {
        "id": "5WRmCUIwVnMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "l6D6e28tUR8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"1D_Advection_Sols_beta0.1_reduced.hdf5\"\n",
        "base_path = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Training settings\n",
        "if_training = True\n",
        "continue_training = False\n",
        "num_workers = 0\n",
        "batch_size = 50\n",
        "initial_step = 10\n",
        "t_train = 200\n",
        "epochs = 500\n",
        "learning_rate = 1e-3\n",
        "scheduler_step = 100\n",
        "scheduler_gamma = 0.5\n",
        "model_update = 1"
      ],
      "metadata": {
        "id": "vusHAhyHUR5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 1\n",
        "size = 128  # Spatial grid size (important for CNO)\n",
        "\n",
        "# New CNO specific parameters (replacing FNO parameters)\n",
        "N_layers = 4\n",
        "N_res = 4\n",
        "N_res_neck = 4\n",
        "channel_multiplier = 16\n",
        "\n",
        "# Dataset preprocessing options\n",
        "single_file = True\n",
        "reduced_resolution = 1\n",
        "reduced_resolution_t = 1\n",
        "reduced_batch = 1\n",
        "\n",
        "# Plotting and bounds\n",
        "plot = False\n",
        "training_type = \"autoregressive\""
      ],
      "metadata": {
        "id": "11uwyEiqUR3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = filename[:-5] + \"_CNO\"\n",
        "model_path = model_name + \".pt\""
      ],
      "metadata": {
        "id": "zUFbrkdbU8z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = FNODatasetSingle(\n",
        "    filename,\n",
        "    reduced_resolution=reduced_resolution,\n",
        "    reduced_resolution_t=reduced_resolution_t,\n",
        "    reduced_batch=reduced_batch,\n",
        "    initial_step=initial_step,\n",
        "    saved_folder=base_path,\n",
        ")\n",
        "\n",
        "val_data = FNODatasetSingle(\n",
        "    filename,\n",
        "    reduced_resolution=reduced_resolution,\n",
        "    reduced_resolution_t=reduced_resolution_t,\n",
        "    reduced_batch=reduced_batch,\n",
        "    initial_step=initial_step,\n",
        "    if_test=True,\n",
        "    saved_folder=base_path,\n",
        ")"
      ],
      "metadata": {
        "id": "FzospQ51U8xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "3l_Jb32lU8uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, _data, _grid = next(iter(val_loader))\n",
        "t_train = min(t_train, _data.shape[-2])\n",
        "size = _data.shape[1]"
      ],
      "metadata": {
        "id": "WyYdxqNdU8rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNO1d(\n",
        "    num_channels=num_channels,\n",
        "    width=channel_multiplier,  # Use channel_multiplier as width\n",
        "    initial_step=initial_step,\n",
        "    size=size,\n",
        "    N_layers=N_layers,\n",
        "    N_res=N_res,\n",
        "    N_res_neck=N_res_neck,\n",
        "    channel_multiplier=channel_multiplier,\n",
        "    use_bn=True\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "fZxJuLtwVx6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
        "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
        "loss_val_min = np.inf\n",
        "start_epoch = 0"
      ],
      "metadata": {
        "id": "z73mAtuKVx3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metric_func(\n",
        "    pred, target, if_mean=True, Lx=1.0, Ly=1.0, Lz=1.0, iLow=4, iHigh=12, initial_step=1\n",
        "):\n",
        "    \"\"\"\n",
        "    code for calculate metrics discussed in the Brain-storming session\n",
        "    RMSE, normalized RMSE, max error, RMSE at the boundaries, conserved variables, RMSE in Fourier space, temporal sensitivity\n",
        "    \"\"\"\n",
        "    pred, target = pred.to(device), target.to(device)\n",
        "    # (batch, nx^i..., timesteps, nc)\n",
        "    # slice out `initial context` timesteps\n",
        "    pred = pred[..., initial_step:, :]\n",
        "    target = target[..., initial_step:, :]\n",
        "    idxs = target.size()\n",
        "    if len(idxs) == 4:  # 1D\n",
        "        pred = pred.permute(0, 3, 1, 2)\n",
        "        target = target.permute(0, 3, 1, 2)\n",
        "\n",
        "    idxs = target.size()\n",
        "    nb, nc, nt = idxs[0], idxs[1], idxs[-1]\n",
        "\n",
        "    # RMSE\n",
        "    err_mean = torch.sqrt(\n",
        "        torch.mean(\n",
        "            (pred.view([nb, nc, -1, nt]) - target.view([nb, nc, -1, nt])) ** 2, dim=2\n",
        "        )\n",
        "    )\n",
        "    err_RMSE = torch.mean(err_mean, axis=0)\n",
        "    nrm = torch.sqrt(torch.mean(target.view([nb, nc, -1, nt]) ** 2, dim=2))\n",
        "    err_nRMSE = torch.mean(err_mean / nrm, dim=0)\n",
        "\n",
        "    err_CSV = torch.sqrt(\n",
        "        torch.mean(\n",
        "            (\n",
        "                torch.sum(pred.view([nb, nc, -1, nt]), dim=2)\n",
        "                - torch.sum(target.view([nb, nc, -1, nt]), dim=2)\n",
        "            )\n",
        "            ** 2,\n",
        "            dim=0,\n",
        "        )\n",
        "    )\n",
        "    if len(idxs) == 4:\n",
        "        nx = idxs[2]\n",
        "        err_CSV /= nx\n",
        "\n",
        "    # worst case in all the data\n",
        "    err_Max = torch.max(\n",
        "        torch.max(\n",
        "            torch.abs(pred.view([nb, nc, -1, nt]) - target.view([nb, nc, -1, nt])),\n",
        "            dim=2,\n",
        "        )[0],\n",
        "        dim=0,\n",
        "    )[0]\n",
        "\n",
        "    if len(idxs) == 4:  # 1D\n",
        "        err_BD = (pred[:, :, 0, :] - target[:, :, 0, :]) ** 2\n",
        "        err_BD += (pred[:, :, -1, :] - target[:, :, -1, :]) ** 2\n",
        "        err_BD = torch.mean(torch.sqrt(err_BD / 2.0), dim=0)\n",
        "\n",
        "    if len(idxs) == 4:  # 1D\n",
        "        nx = idxs[2]\n",
        "        pred_F = torch.fft.rfft(pred, dim=2)\n",
        "        target_F = torch.fft.rfft(target, dim=2)\n",
        "        _err_F = (\n",
        "            torch.sqrt(torch.mean(torch.abs(pred_F - target_F) ** 2, axis=0)) / nx * Lx\n",
        "        )\n",
        "\n",
        "    err_F = torch.zeros([nc, 3, nt]).to(device)\n",
        "    err_F[:, 0] += torch.mean(_err_F[:, :iLow], dim=1)  # low freq\n",
        "    err_F[:, 1] += torch.mean(_err_F[:, iLow:iHigh], dim=1)  # middle freq\n",
        "    err_F[:, 2] += torch.mean(_err_F[:, iHigh:], dim=1)  # high freq\n",
        "\n",
        "    if if_mean:\n",
        "        return (\n",
        "            torch.mean(err_RMSE, dim=[0, -1]),\n",
        "            torch.mean(err_nRMSE, dim=[0, -1]),\n",
        "            torch.mean(err_CSV, dim=[0, -1]),\n",
        "            torch.mean(err_Max, dim=[0, -1]),\n",
        "            torch.mean(err_BD, dim=[0, -1]),\n",
        "            torch.mean(err_F, dim=[0, -1]),\n",
        "        )\n",
        "    return err_RMSE, err_nRMSE, err_CSV, err_Max, err_BD, err_F"
      ],
      "metadata": {
        "id": "74j81YIlVx1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(\n",
        "    val_loader,\n",
        "    model,\n",
        "    Lx,\n",
        "    Ly,\n",
        "    Lz,\n",
        "    plot,\n",
        "    channel_plot,\n",
        "    model_name,\n",
        "    x_min,\n",
        "    x_max,\n",
        "    y_min,\n",
        "    y_max,\n",
        "    t_min,\n",
        "    t_max,\n",
        "    mode=\"CNO\",  # Changed default to CNO\n",
        "    initial_step=None,\n",
        "):\n",
        "    if mode == \"CNO\":  # Update to use CNO instead of FNO\n",
        "        with torch.no_grad():\n",
        "            itot = 0\n",
        "            for itot, (xx, yy, grid) in enumerate(val_loader):\n",
        "                xx = xx.to(device)  # noqa: PLW2901\n",
        "                yy = yy.to(device)  # noqa: PLW2901\n",
        "                grid = grid.to(device)  # noqa: PLW2901\n",
        "\n",
        "                pred = yy[..., :initial_step, :]\n",
        "                inp_shape = list(xx.shape)\n",
        "                inp_shape = inp_shape[:-2]\n",
        "                inp_shape.append(-1)\n",
        "\n",
        "                for _t in range(initial_step, yy.shape[-2]):\n",
        "                    inp = xx.reshape(inp_shape)\n",
        "                    im = model(inp, grid)\n",
        "                    pred = torch.cat((pred, im), -2)\n",
        "                    xx = torch.cat((xx[..., 1:, :], im), dim=-2)  # noqa: PLW2901\n",
        "\n",
        "                (\n",
        "                    _err_RMSE,\n",
        "                    _err_nRMSE,\n",
        "                    _err_CSV,\n",
        "                    _err_Max,\n",
        "                    _err_BD,\n",
        "                    _err_F,\n",
        "                ) = metric_func(\n",
        "                    pred,\n",
        "                    yy,\n",
        "                    if_mean=True,\n",
        "                    Lx=Lx,\n",
        "                    Ly=Ly,\n",
        "                    Lz=Lz,\n",
        "                    initial_step=initial_step,\n",
        "                )\n",
        "                if itot == 0:\n",
        "                    err_RMSE, err_nRMSE, err_CSV, err_Max, err_BD, err_F = (\n",
        "                        _err_RMSE,\n",
        "                        _err_nRMSE,\n",
        "                        _err_CSV,\n",
        "                        _err_Max,\n",
        "                        _err_BD,\n",
        "                        _err_F,\n",
        "                    )\n",
        "                    pred_plot = pred[:1]\n",
        "                    target_plot = yy[:1]\n",
        "                    val_l2_time = torch.zeros(yy.shape[-2]).to(device)\n",
        "                else:\n",
        "                    err_RMSE += _err_RMSE\n",
        "                    err_nRMSE += _err_nRMSE\n",
        "                    err_CSV += _err_CSV\n",
        "                    err_Max += _err_Max\n",
        "                    err_BD += _err_BD\n",
        "                    err_F += _err_F\n",
        "\n",
        "                    mean_dim = list(range(len(yy.shape) - 2))\n",
        "                    mean_dim.append(-1)\n",
        "                    mean_dim = tuple(mean_dim)\n",
        "                    val_l2_time += torch.sqrt(\n",
        "                        torch.mean((pred - yy) ** 2, dim=mean_dim)\n",
        "                    )\n",
        "\n",
        "    err_RMSE = np.array(err_RMSE.data.cpu() / itot)\n",
        "    err_nRMSE = np.array(err_nRMSE.data.cpu() / itot)\n",
        "    err_CSV = np.array(err_CSV.data.cpu() / itot)\n",
        "    err_Max = np.array(err_Max.data.cpu() / itot)\n",
        "    err_BD = np.array(err_BD.data.cpu() / itot)\n",
        "    err_F = np.array(err_F.data.cpu() / itot)\n",
        "    logger.info(f\"RMSE: {err_RMSE:.5f}\")\n",
        "    logger.info(f\"normalized RMSE: {err_nRMSE:.5f}\")\n",
        "    logger.info(f\"RMSE of conserved variables: {err_CSV:.5f}\")\n",
        "    logger.info(f\"Maximum value of rms error: {err_Max:.5f}\")\n",
        "    logger.info(f\"RMSE at boundaries: {err_BD:.5f}\")\n",
        "    logger.info(f\"RMSE in Fourier space: {err_F}\")\n",
        "\n",
        "    val_l2_time = val_l2_time / itot\n",
        "\n",
        "    return err_RMSE, err_nRMSE, err_CSV, err_Max, err_BD, err_F"
      ],
      "metadata": {
        "id": "VE110_EWV-PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_l2_step, train_l2_full = 0, 0\n",
        "\n",
        "    for xx, yy, grid in train_loader:\n",
        "        loss = 0\n",
        "        xx, yy, grid = xx.to(device), yy.to(device), grid.to(device)\n",
        "        pred = yy[..., :initial_step, :]\n",
        "        inp_shape = list(xx.shape[:-2]) + [-1]\n",
        "\n",
        "        for t in range(initial_step, t_train):\n",
        "            inp = xx.reshape(inp_shape)\n",
        "            y = yy[..., t : t + 1, :]\n",
        "            im = model(inp, grid)\n",
        "            _batch = im.size(0)\n",
        "            loss += loss_fn(im.reshape(_batch, -1), y.reshape(_batch, -1))\n",
        "            pred = torch.cat((pred, im), -2)\n",
        "            xx = torch.cat((xx[..., 1:, :], im), dim=-2)\n",
        "\n",
        "        train_l2_step += loss.item()\n",
        "        _batch = yy.size(0)\n",
        "        _yy = yy[..., :t_train, :]\n",
        "        l2_full = loss_fn(pred.reshape(_batch, -1), _yy.reshape(_batch, -1))\n",
        "        train_l2_full += l2_full.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if ep % model_update == 0:\n",
        "        val_l2_step, val_l2_full = 0, 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xx, yy, grid in val_loader:\n",
        "                loss = 0\n",
        "                xx, yy, grid = xx.to(device), yy.to(device), grid.to(device)\n",
        "                pred = yy[..., :initial_step, :]\n",
        "                inp_shape = list(xx.shape[:-2]) + [-1]\n",
        "\n",
        "                for t in range(initial_step, yy.shape[-2]):\n",
        "                    inp = xx.reshape(inp_shape)\n",
        "                    y = yy[..., t : t + 1, :]\n",
        "                    im = model(inp, grid)\n",
        "                    _batch = im.size(0)\n",
        "                    loss += loss_fn(im.reshape(_batch, -1), y.reshape(_batch, -1))\n",
        "                    pred = torch.cat((pred, im), -2)\n",
        "                    xx = torch.cat((xx[..., 1:, :], im), dim=-2)\n",
        "\n",
        "                val_l2_step += loss.item()\n",
        "                _pred = pred[..., initial_step:t_train, :]\n",
        "                _yy = yy[..., initial_step:t_train, :]\n",
        "                val_l2_full += loss_fn(_pred.reshape(_batch, -1), _yy.reshape(_batch, -1)).item()\n",
        "\n",
        "            if val_l2_full < loss_val_min:\n",
        "                loss_val_min = val_l2_full\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"epoch\": ep,\n",
        "                        \"model_state_dict\": model.state_dict(),\n",
        "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                        \"loss\": loss_val_min,\n",
        "                    },\n",
        "                    model_path,\n",
        "                )\n",
        "    t2 = default_timer()\n",
        "    scheduler.step()\n",
        "    print(\"epoch: {0}, loss: {1:.5f}, t2-t1: {2:.5f}, trainL2: {3:.5f}, testL2: {4:.5f}\".format(ep, loss.item(), t2 - t1, train_l2_full, val_l2_full))\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "QACvMbd5V-MW",
        "outputId": "d5a7f2c5-8421-4027-d8d9-666e3c3f86d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3269d8d61f75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXHp6_WCV-Jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}